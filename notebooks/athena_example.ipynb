{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile ../athena_example/athena_example.py\n",
    "\n",
    "\"\"\"\n",
    "This is an example of working with very large data. There are about\n",
    "700,000 unduplicated donors in this database of Illinois political\n",
    "campaign contributions.\n",
    "\n",
    "With such a large set of input data, we cannot store all the comparisons\n",
    "we need to make in memory. Instead, we will read the pairs on demand\n",
    "from the Athena database.\n",
    "\n",
    "__Note:__ You will need to run `python athena_init_db.py`\n",
    "before running this script. See the annotates source for\n",
    "[athena_init_db.py](athena_init_db.html)\n",
    "\n",
    "For smaller datasets (<10,000), see our\n",
    "[csv_example](csv_example.html)\n",
    "\"\"\"\n",
    "\n",
    "# There is a little bit difference between the result \n",
    "# of this module and the athena one. The reason is due to\n",
    "# Some special (and mostly erroneous) characters, such as \\a .. \n",
    "# Which are dealt with differently by athena and athena/panda\n",
    "\n",
    "import sys\n",
    "import os\n",
    "import itertools\n",
    "import time\n",
    "import logging\n",
    "import optparse\n",
    "import locale\n",
    "import json\n",
    "from io import StringIO\n",
    "import csv\n",
    "import pandas as pd\n",
    "\n",
    "import boto3\n",
    "import dedupe\n",
    "import dedupe.backport\n",
    "sys.path.insert(0, '../athena_example/')\n",
    "import config\n",
    "sys.path.insert(0, '../athena_example/')\n",
    "import athenautils\n",
    "\n",
    "def cursor_execute(query, database):\n",
    "    '''\n",
    "    The MySQL compatible Cursor\n",
    "    '''\n",
    "    return athenautils.cursor_execute(query, database=database, \n",
    "                                      cursortype='tuple', buffersize=config.BUFFERSIZE,\n",
    "                                      escapechar=None, keep_default_na=False, na_values=[''])\n",
    "\n",
    "def dict_cursor_execute(query, database):\n",
    "    '''\n",
    "    The MySQL compatible DicCursor\n",
    "    '''\n",
    "    return athenautils.cursor_execute(query, database=database, \n",
    "                                      cursortype='dict', buffersize=config.BUFFERSIZE,\n",
    "                                      escapechar=None, keep_default_na=False, na_values=[''])\n",
    "def record_pairs(result_set):\n",
    "    for i, row in enumerate(result_set):\n",
    "        a_record_id, a_record, b_record_id, b_record = row\n",
    "        record_a = (a_record_id, json.loads(a_record))\n",
    "        record_b = (b_record_id, json.loads(b_record))\n",
    "\n",
    "        yield record_a, record_b\n",
    "\n",
    "        if i % 10000 == 0:\n",
    "            print(i)\n",
    "\n",
    "\n",
    "def cluster_ids(clustered_dupes):\n",
    "\n",
    "    for cluster, scores in clustered_dupes:\n",
    "        cluster_id = cluster[0]\n",
    "        for donor_id, score in zip(cluster, scores):\n",
    "            yield donor_id, cluster_id, score\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "\n",
    "    ## Logging\n",
    "\n",
    "    # Dedupe uses Python logging to show or suppress verbose output. Added\n",
    "    # for convenience.  To enable verbose output, run `python\n",
    "    # examples/athena_example/athena_example.py -v`\n",
    "    \n",
    "    optp = optparse.OptionParser()\n",
    "    optp.add_option('-v', '--verbose', dest='verbose', action='count',\n",
    "                    help='Increase verbosity (specify multiple times for more)'\n",
    "                    )\n",
    "    (opts, args) = optp.parse_args()\n",
    "    log_level = logging.WARNING\n",
    "    if opts.verbose:\n",
    "        if opts.verbose == 1:\n",
    "            log_level = logging.INFO\n",
    "        elif opts.verbose >= 2:\n",
    "            log_level = logging.DEBUG\n",
    "\n",
    "\n",
    "    logging.getLogger().setLevel(log_level)\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "    settings_file = 'athena_example_settings'\n",
    "    training_file = 'athena_example_training.json'\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    # We'll be using variations on this following select statement to pull\n",
    "    # in campaign donor info.\n",
    "    #\n",
    "    # We did a fair amount of preprocessing of the fields in\n",
    "    # `athena_init_db.py`    \n",
    "    DONOR_SELECT = \"\"\"SELECT donor_id, city, name, zip, state, address\n",
    "                      from as_processed_donors\"\"\"\n",
    "\n",
    "    # ## Training\n",
    "\n",
    "    if os.path.exists(settings_file):\n",
    "        print('reading from ', settings_file)\n",
    "        with open(settings_file, 'rb') as sf:\n",
    "            deduper = dedupe.StaticDedupe(sf, num_cores=4)\n",
    "    else:\n",
    "        # Define the fields dedupe will pay attention to\n",
    "        #\n",
    "        # The address, city, and zip fields are often missing, so we'll\n",
    "        # tell dedupe that, and we'll learn a model that take that into\n",
    "        # account\n",
    "        fields = [{'field': 'name', 'type': 'String'},\n",
    "                  {'field': 'address', 'type': 'String',\n",
    "                   'has missing': True},\n",
    "                  {'field': 'city', 'type': 'ShortString', 'has missing': True},\n",
    "                  {'field': 'state', 'type': 'ShortString', 'has missing': True},\n",
    "                  {'field': 'zip', 'type': 'ShortString', 'has missing': True},\n",
    "                  ]\n",
    "\n",
    "        # Create a new deduper object and pass our data model to it.\n",
    "        deduper = dedupe.Dedupe(fields, num_cores=4)\n",
    "\n",
    "        # We will sample pairs from the entire donor table for training\n",
    "        cur = dict_cursor_execute(DONOR_SELECT, database=config.DATABASE)\n",
    "        temp_d = {i: row for i, row in enumerate(cur)}\n",
    "            \n",
    "\n",
    "        # If we have training data saved from a previous run of dedupe,\n",
    "        # look for it an load it in.\n",
    "        #\n",
    "        # __Note:__ if you want to train from\n",
    "        # scratch, delete the training_file\n",
    "        if os.path.exists(training_file):\n",
    "            print('reading labeled examples from ', training_file)\n",
    "            with open(training_file) as tf:\n",
    "                deduper.prepare_training(temp_d, training_file=tf)\n",
    "        else:\n",
    "            deduper.prepare_training(temp_d)\n",
    "\n",
    "        del temp_d\n",
    "\n",
    "        # ## Active learning\n",
    "\n",
    "        print('starting active labeling...')\n",
    "        # Starts the training loop. Dedupe will find the next pair of records\n",
    "        # it is least certain about and ask you to label them as duplicates\n",
    "        # or not.\n",
    "\n",
    "        # use 'y', 'n' and 'u' keys to flag duplicates\n",
    "        # press 'f' when you are finished\n",
    "        dedupe.convenience.console_label(deduper)\n",
    "        # When finished, save our labeled, training pairs to disk\n",
    "        with open(training_file, 'w') as tf:\n",
    "            deduper.write_training(tf)\n",
    "\n",
    "        # Notice our the argument here\n",
    "        #\n",
    "        # `recall` is the proportion of true dupes pairs that the learned\n",
    "        # rules must cover. You may want to reduce this if your are making\n",
    "        # too many blocks and too many comparisons.\n",
    "        deduper.train(recall=0.90)\n",
    "\n",
    "        with open(settings_file, 'wb') as sf:\n",
    "            deduper.write_settings(sf)\n",
    "\n",
    "        # We can now remove some of the memory hobbing objects we used\n",
    "        # for training\n",
    "        deduper.cleanup_training()\n",
    "\n",
    "    # ## Blocking\n",
    "\n",
    "    print('blocking...')\n",
    "\n",
    "    # To run blocking on such a large set of data, we create a separate table\n",
    "    # that contains blocking keys and record ids\n",
    "    print('creating as_blocking_map database')\n",
    "    athenautils.drop_external_table(\"as_blocking_map\", \n",
    "                                    location = 's3://{}/{}'.format(config.DATABASE_BUCKET, config.DATABASE_ROOT_KEY+'as_blocking_map'),\n",
    "                                    database=config.DATABASE)\n",
    "\n",
    "    q=\"\"\"\n",
    "    CREATE EXTERNAL TABLE as_blocking_map     \n",
    "        (block_key VARCHAR(200), donor_id INTEGER)\n",
    "    ROW FORMAT DELIMITED\n",
    "      FIELDS TERMINATED BY '\\t'\n",
    "      LINES TERMINATED BY '\\n'  \n",
    "    LOCATION\n",
    "        's3://{}/{}' \n",
    "    TBLPROPERTIES (\n",
    "        'classification'='csv', \n",
    "        --'skip.header.line.count'='1',  \n",
    "        'serialization.null.format'='')\n",
    "    \"\"\".format(config.DATABASE_BUCKET, config.DATABASE_ROOT_KEY+'as_blocking_map') \n",
    "    athenautils.athena_start_query(q, database=config.DATABASE)\n",
    "\n",
    "    # If dedupe learned a Index Predicate, we have to take a pass\n",
    "    # through the data and create indices.\n",
    "    print('creating inverted index')\n",
    "\n",
    "    # Armin: \n",
    "    # This never runs, index_fields is empty, possible bug?\n",
    "    for field in deduper.fingerprinter.index_fields:\n",
    "        q = \"\"\"\n",
    "        SELECT DISTINCT {field} FROM as_processed_donors\n",
    "        WHERE {field} IS NOT NULL\n",
    "        \"\"\".format(field=field)\n",
    "        cur = dict_cursor_execute(q, databse=config.DATABASE)\n",
    "        field_data = (row[field] for row in cur)\n",
    "        deduper.fingerprinter.index(field_data, field)\n",
    "     \n",
    "\n",
    "    # Now we are ready to write our blocking map table by creating a\n",
    "    # generator that yields unique `(block_key, donor_id)` tuples.\n",
    "    print('writing blocking map')\n",
    "    \n",
    "    read_cur  = dict_cursor_execute(DONOR_SELECT, database=config.DATABASE)\n",
    "    full_data = ((row['donor_id'], row) for row in read_cur)\n",
    "\n",
    "    b_data = deduper.fingerprinter(full_data)\n",
    "    athenautils.write_many(b_data, \n",
    "                           filename='s3://{}/{}'.format(config.DATABASE_BUCKET, config.DATABASE_ROOT_KEY+'as_blocking_map/blocking.csv'))\n",
    "\n",
    "\n",
    "    # select unique pairs to compare\n",
    "    q=\"\"\"\n",
    "        SELECT a.donor_id,\n",
    "            json_format(CAST (MAP(ARRAY['city', 'name', 'zip', 'state', 'address'],\n",
    "                                  ARRAY[ a.city, a.name, a.zip, a.state, a.address])\n",
    "                        AS JSON)),\n",
    "            b.donor_id,\n",
    "            json_format(CAST (MAP(ARRAY['city', 'name', 'zip', 'state', 'address'], \n",
    "                      ARRAY[ b.city, b.name, b.zip, b.state, b.address])\n",
    "                  AS JSON))\n",
    "        FROM (SELECT DISTINCT l.donor_id as east, r.donor_id as west\n",
    "             from as_blocking_map as l\n",
    "             INNER JOIN as_blocking_map as r\n",
    "             using (block_key)\n",
    "             where l.donor_id < r.donor_id) ids\n",
    "        INNER JOIN as_processed_donors a on ids.east=a.donor_id\n",
    "        INNER JOIN as_processed_donors b on ids.west=b.donor_id\n",
    "       \"\"\"\n",
    "    read_cur = cursor_execute(q, database=config.DATABASE)\n",
    "\n",
    "\n",
    "    # ## Clustering\n",
    "\n",
    "    print('clustering...')\n",
    "    clustered_dupes = deduper.cluster(deduper.score(record_pairs(read_cur)),\n",
    "                                      threshold=0.5)\n",
    "\n",
    "#     athenautils.athena_start_query(\"DROP TABLE IF EXISTS as_entity_map\", database=config.DATABASE)\n",
    "    athenautils.drop_external_table(\"as_entity_map\", \n",
    "                                    location='s3://{}/{}'.format(config.DATABASE_BUCKET, config.DATABASE_ROOT_KEY+'as_entity_map/'), \n",
    "                                    database=config.DATABASE)\n",
    "    \n",
    "    print('creating as_entity_map database')\n",
    "    q=\"\"\"\n",
    "    CREATE EXTERNAL TABLE as_entity_map     \n",
    "        (donor_id INTEGER, canon_id INTEGER, \n",
    "         cluster_score FLOAT)\n",
    "    ROW FORMAT DELIMITED\n",
    "      FIELDS TERMINATED BY '\\t'\n",
    "      LINES TERMINATED BY '\\n'  \n",
    "    LOCATION\n",
    "        's3://{}/{}' \n",
    "    TBLPROPERTIES (\n",
    "        'classification'='csv', \n",
    "        --'skip.header.line.count'='1',  \n",
    "        'serialization.null.format'='')\n",
    "    \"\"\".format(config.DATABASE_BUCKET, config.DATABASE_ROOT_KEY+'as_entity_map') \n",
    "    athenautils.athena_start_query(q, database=config.DATABASE) \n",
    "\n",
    "    athenautils.write_many(cluster_ids(clustered_dupes),\n",
    "                          filename='s3://{}/{}'.format(config.DATABASE_BUCKET, config.DATABASE_ROOT_KEY+'as_entity_map/entity_map.csv'))\n",
    "\n",
    "    # Print out the number of duplicates found\n",
    "    print('# duplicate sets')\n",
    "\n",
    "    # ## Payoff\n",
    "\n",
    "    # With all this done, we can now begin to ask interesting questions\n",
    "    # of the data\n",
    "    #\n",
    "    # For example, let's see who the top 10 donors are.\n",
    "\n",
    "    locale.setlocale(locale.LC_ALL, 'en_CA.UTF-8')  # for pretty printing numbers\n",
    "    \n",
    "    athenautils.athena_start_query(\"DROP TABLE IF EXISTS as_e_map\", database=config.DATABASE)\n",
    "    \n",
    "    q = \"\"\"\n",
    "        CREATE TABLE as_e_map as \n",
    "        SELECT COALESCE(canon_id, as_entity_map.donor_id) AS canon_id, as_entity_map.donor_id \n",
    "        FROM as_entity_map \n",
    "        RIGHT JOIN as_donors USING(donor_id)        \n",
    "        \"\"\"    \n",
    "    athenautils.athena_start_query(q, database=config.DATABASE)\n",
    "    \n",
    "    q = \"\"\"\n",
    "        SELECT array_join(filter(array[as_donors.first_name, as_donors.last_name], x-> x IS NOT NULL), ' ') AS name,   \n",
    "            donation_totals.totals AS totals \n",
    "        FROM as_donors INNER JOIN \n",
    "            (SELECT canon_id, SUM(cast (amount as double)) AS totals \n",
    "            FROM as_contributions INNER JOIN as_e_map \n",
    "            USING (donor_id) \n",
    "            GROUP BY (canon_id) \n",
    "            ORDER BY totals \n",
    "            DESC LIMIT 10) \n",
    "            AS donation_totals \n",
    "        ON as_donors.donor_id = donation_totals.canon_id\n",
    "        ORDER BY totals DESC\n",
    "    \"\"\"\n",
    "    cur = dict_cursor_execute(q, database=config.DATABASE)\n",
    "\n",
    "    print(\"Top Donors (deduped)\")\n",
    "    for row in cur:\n",
    "        row['totals'] = locale.currency(row['totals'], grouping=True)\n",
    "        print('%(totals)20s: %(name)s' % row)\n",
    "\n",
    "    # Compare this to what we would have gotten if we hadn't done any\n",
    "    # deduplication\n",
    "    q = \"\"\"\n",
    "        with donorscontributions as(\n",
    "\n",
    "            SELECT as_donors.donor_id, \n",
    "                array_join(filter(array[as_donors.first_name, as_donors.last_name], x-> x IS NOT NULL), ' ') AS name,\n",
    "                cast(as_contributions.amount as double) as amount\n",
    "            FROM as_donors INNER JOIN as_contributions \n",
    "                USING (donor_id) \n",
    "            )\n",
    "        SELECT name, sum(amount) AS totals  \n",
    "        FROM donorscontributions\n",
    "        GROUP BY donor_id, name\n",
    "        ORDER BY totals DESC \n",
    "        LIMIT 10\n",
    "    \"\"\"\n",
    "    cur = dict_cursor_execute(q, database=config.DATABASE)\n",
    "\n",
    "    print(\"Top Donors (raw)\")\n",
    "    for row in cur:\n",
    "        row['totals'] = locale.currency(row['totals'], grouping=True)\n",
    "        print('%(totals)20s: %(name)s' % row)\n",
    "\n",
    "    print('ran in', time.time() - start_time, 'seconds')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python ../athena_example/athena_example.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
