{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load ../mysql_example/mysql_example.py\n",
    "#!/usr/bin/python\n",
    "\n",
    "\"\"\"\n",
    "This is an example of working with very large data. There are about\n",
    "700,000 unduplicated donors in this database of Illinois political\n",
    "campaign contributions.\n",
    "\n",
    "With such a large set of input data, we cannot store all the comparisons\n",
    "we need to make in memory. Instead, we will read the pairs on demand\n",
    "from the MySQL database.\n",
    "\n",
    "__Note:__ You will need to run `python mysql_init_db.py`\n",
    "before running this script. See the annotates source for\n",
    "[mysql_init_db.py](mysql_init_db.html)\n",
    "\n",
    "For smaller datasets (<10,000), see our\n",
    "[csv_example](csv_example.html)\n",
    "\"\"\"\n",
    "\n",
    "# There is a little bit difference between the result \n",
    "# of this module and the mysql one. The reason is due to\n",
    "# Some special (and mostly erroneous) characters, such as \\a .. \n",
    "# Which are dealt with differently by mysql and athena/panda\n",
    "\n",
    "import sys\n",
    "import os\n",
    "import itertools\n",
    "import time\n",
    "import logging\n",
    "import optparse\n",
    "import locale\n",
    "import json\n",
    "from io import StringIO\n",
    "import csv\n",
    "import pandas as pd\n",
    "\n",
    "import boto3\n",
    "import dedupe\n",
    "import dedupe.backport\n",
    "sys.path.insert(0, '../athena_example/')\n",
    "import config\n",
    "sys.path.insert(0, '../athena_example/')\n",
    "import utils\n",
    "\n",
    "def as_pandas(query, **kwrgs):\n",
    "    df = utils.athena_to_panda(query, escapechar=None, keep_default_na=False, na_values=[''], **kwrgs)\n",
    "    return df.where(pd.notnull(df), None)\n",
    "\n",
    "def record_pairs(result_set):\n",
    "    for i, row in enumerate(result_set):\n",
    "        a_record_id, a_record, b_record_id, b_record = row\n",
    "        record_a = (a_record_id, json.loads(a_record))\n",
    "        record_b = (b_record_id, json.loads(b_record))\n",
    "\n",
    "        yield record_a, record_b\n",
    "\n",
    "        if i % 10000 == 0:\n",
    "            print(i)\n",
    "\n",
    "\n",
    "def cluster_ids(clustered_dupes):\n",
    "\n",
    "    for cluster, scores in clustered_dupes:\n",
    "        cluster_id = cluster[0]\n",
    "        for donor_id, score in zip(cluster, scores):\n",
    "            yield donor_id, cluster_id, score\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "\n",
    "    # ## Logging\n",
    "\n",
    "    # Dedupe uses Python logging to show or suppress verbose output. Added\n",
    "    # for convenience.  To enable verbose output, run `python\n",
    "    # examples/mysql_example/mysql_example.py -v`\n",
    "    \n",
    "    optp = optparse.OptionParser()\n",
    "    optp.add_option('-v', '--verbose', dest='verbose', action='count',\n",
    "                    help='Increase verbosity (specify multiple times for more)'\n",
    "                    )\n",
    "    (opts, args) = optp.parse_args()\n",
    "    log_level = logging.WARNING\n",
    "    if opts.verbose:\n",
    "        if opts.verbose == 1:\n",
    "            log_level = logging.INFO\n",
    "        elif opts.verbose >= 2:\n",
    "            log_level = logging.DEBUG\n",
    "\n",
    "\n",
    "    logging.getLogger().setLevel(log_level)\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "    settings_file = 'mysql_example_settings'\n",
    "    training_file = 'mysql_example_training.json'\n",
    "\n",
    "    start_time = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    # We'll be using variations on this following select statement to pull\n",
    "    # in campaign donor info.\n",
    "    #\n",
    "    # We did a fair amount of preprocessing of the fields in\n",
    "    # `mysql_init_db.py`    \n",
    "    DONOR_SELECT = \"SELECT donor_id, city, name, zip, state, address \" \\\n",
    "                   \"from processed_donors\"\n",
    "\n",
    "    # ## Training\n",
    "\n",
    "    if os.path.exists(settings_file):\n",
    "        print('reading from ', settings_file)\n",
    "        with open(settings_file, 'rb') as sf:\n",
    "            deduper = dedupe.StaticDedupe(sf, num_cores=4)\n",
    "    else:\n",
    "        # Define the fields dedupe will pay attention to\n",
    "        #\n",
    "        # The address, city, and zip fields are often missing, so we'll\n",
    "        # tell dedupe that, and we'll learn a model that take that into\n",
    "        # account\n",
    "        fields = [{'field': 'name', 'type': 'String'},\n",
    "                  {'field': 'address', 'type': 'String',\n",
    "                   'has missing': True},\n",
    "                  {'field': 'city', 'type': 'ShortString', 'has missing': True},\n",
    "                  {'field': 'state', 'type': 'ShortString', 'has missing': True},\n",
    "                  {'field': 'zip', 'type': 'ShortString', 'has missing': True},\n",
    "                  ]\n",
    "\n",
    "        # Create a new deduper object and pass our data model to it.\n",
    "        deduper = dedupe.Dedupe(fields, num_cores=4)\n",
    "\n",
    "        # We will sample pairs from the entire donor table for training\n",
    "#         with read_con.cursor() as cur:\n",
    "\n",
    "        # Armin: The problem is the donor_id, it's numpy's int64, should be converted to int! \n",
    "        # But for that, astype doesn't work, and a loop on temp_d is slow, so for now let's just use str\n",
    "#         with conn.cursor(PandasCursor, schema_name=schema_name) as cursor:\n",
    "        temp_df = as_pandas(DONOR_SELECT)\n",
    "        temp_d = temp_df.to_dict('index')\n",
    "            \n",
    "\n",
    "        # If we have training data saved from a previous run of dedupe,\n",
    "        # look for it an load it in.\n",
    "        #\n",
    "        # __Note:__ if you want to train from\n",
    "        # scratch, delete the training_file\n",
    "        if os.path.exists(training_file):\n",
    "            print('reading labeled examples from ', training_file)\n",
    "            with open(training_file) as tf:\n",
    "                deduper.prepare_training(temp_d, training_file=tf)\n",
    "        else:\n",
    "            deduper.prepare_training(temp_d)\n",
    "\n",
    "        del temp_d\n",
    "\n",
    "        # ## Active learning\n",
    "\n",
    "        print('starting active labeling...')\n",
    "        # Starts the training loop. Dedupe will find the next pair of records\n",
    "        # it is least certain about and ask you to label them as duplicates\n",
    "        # or not.\n",
    "\n",
    "        # use 'y', 'n' and 'u' keys to flag duplicates\n",
    "        # press 'f' when you are finished\n",
    "        dedupe.convenience.console_label(deduper)\n",
    "        # When finished, save our labeled, training pairs to disk\n",
    "        with open(training_file, 'w') as tf:\n",
    "            deduper.write_training(tf)\n",
    "\n",
    "        # Notice our the argument here\n",
    "        #\n",
    "        # `recall` is the proportion of true dupes pairs that the learned\n",
    "        # rules must cover. You may want to reduce this if your are making\n",
    "        # too many blocks and too many comparisons.\n",
    "        deduper.train(recall=0.90)\n",
    "\n",
    "        with open(settings_file, 'wb') as sf:\n",
    "            deduper.write_settings(sf)\n",
    "\n",
    "        # We can now remove some of the memory hobbing objects we used\n",
    "        # for training\n",
    "        deduper.cleanup_training()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    # ## Blocking\n",
    "\n",
    "    print('blocking...')\n",
    "\n",
    "    # To run blocking on such a large set of data, we create a separate table\n",
    "    # that contains blocking keys and record ids\n",
    "    print('creating blocking_map database')\n",
    "    utils.athena_start_query(\"DROP TABLE IF EXISTS blocking_map\")\n",
    "\n",
    "    q='''\n",
    "    CREATE EXTERNAL TABLE blocking_map     \n",
    "        (block_key VARCHAR(200), donor_id INTEGER)\n",
    "    ROW FORMAT DELIMITED\n",
    "      FIELDS TERMINATED BY '\\t'\n",
    "      LINES TERMINATED BY '\\n'  \n",
    "    LOCATION\n",
    "        's3://{}/{}' \n",
    "    TBLPROPERTIES (\n",
    "        'classification'='csv', \n",
    "        --'skip.header.line.count'='1',  \n",
    "        'serialization.null.format'='')\n",
    "    '''.format(config.DATABASE_BUCKET, config.DATABASE_ROOT_KEY+'blocking_map') \n",
    "    utils.athena_start_query(q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    # If dedupe learned a Index Predicate, we have to take a pass\n",
    "    # through the data and create indices.\n",
    "    print('creating inverted index')\n",
    "\n",
    "    # Armin: \n",
    "    # This never runs, index_fields is empty, possible bug?\n",
    "    for field in deduper.fingerprinter.index_fields:\n",
    "        q = '''\n",
    "        SELECT DISTINCT {field} FROM processed_donors \n",
    "        WHERE {field} IS NOT NULL\n",
    "        '''.format(field=field)\n",
    "        cur_df = as_pandas(q)\n",
    "        # Do I need to cast it as a list?\n",
    "        field_data = cur_df[field]\n",
    "        deduper.fingerprinter.index(field_data, field)\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    # Now we are ready to write our blocking map table by creating a\n",
    "    # generator that yields unique `(block_key, donor_id)` tuples.\n",
    "    print('writing blocking map')\n",
    "    \n",
    "\n",
    "    read_cur_dict = as_pandas(DONOR_SELECT).to_dict('records')\n",
    "    full_data = ((row['donor_id'], row) for row in read_cur_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    b_data = deduper.fingerprinter(full_data)\n",
    "    buffer = pd.DataFrame.from_records(b_data).to_csv(index=False, header=False, sep='\\t')    utils.s3.put_object(Bucket=config.DATABASE_BUCKET, Key=config.DATABASE_ROOT_KEY+'blocking_map/blocking.csv', Body=buffer)    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "    # select unique pairs to compare\n",
    "    q='''\n",
    "    SELECT a.donor_id,\n",
    "        json_format(CAST (MAP(ARRAY['city', 'name', 'zip', 'state', 'address'],\n",
    "                              ARRAY[ a.city, a.name, a.zip, a.state, a.address])\n",
    "                    AS JSON)),\n",
    "        b.donor_id,\n",
    "        json_format(CAST (MAP(ARRAY['city', 'name', 'zip', 'state', 'address'], \n",
    "                  ARRAY[ b.city, b.name, b.zip, b.state, b.address])\n",
    "              AS JSON))\n",
    "    FROM (SELECT DISTINCT l.donor_id as east, r.donor_id as west\n",
    "         from blocking_map as l\n",
    "         INNER JOIN blocking_map as r\n",
    "         using (block_key)\n",
    "         where l.donor_id < r.donor_id) ids\n",
    "    INNER JOIN processed_donors a on ids.east=a.donor_id\n",
    "    INNER JOIN processed_donors b on ids.west=b.donor_id\n",
    "    '''\n",
    "    read_cur_dict=as_pandas(q).itertuples(index=False, name=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    # ## Clustering\n",
    "\n",
    "    print('clustering...')\n",
    "    clustered_dupes = deduper.cluster(deduper.score(record_pairs(read_cur_dict)),\n",
    "                                      threshold=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    utils.athena_start_query(\"DROP TABLE IF EXISTS entity_map\")\n",
    "\n",
    "    print('creating entity_map database')\n",
    "    q='''\n",
    "    CREATE EXTERNAL TABLE entity_map     \n",
    "        (donor_id INTEGER, canon_id INTEGER, \n",
    "         cluster_score FLOAT)\n",
    "    ROW FORMAT DELIMITED\n",
    "      FIELDS TERMINATED BY '\\t'\n",
    "      LINES TERMINATED BY '\\n'  \n",
    "    LOCATION\n",
    "        's3://{}/{}' \n",
    "    TBLPROPERTIES (\n",
    "        'classification'='csv', \n",
    "        --'skip.header.line.count'='1',  \n",
    "        'serialization.null.format'='')\n",
    "    '''.format(config.DATABASE_BUCKET, config.DATABASE_ROOT_KEY+'entity_map') \n",
    "    utils.athena_start_query(q) \n",
    "\n",
    "    buffer = pd.DataFrame.from_records(cluster_ids(clustered_dupes)).to_csv(index=False, header=False, sep='\\t')\n",
    "    utils.s3.put_object(Bucket=config.DATABASE_BUCKET, Key=config.DATABASE_ROOT_KEY+'entity_map/entity_map.csv', Body=buffer)    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    # Print out the number of duplicates found\n",
    "    print('# duplicate sets')\n",
    "\n",
    "    # ## Payoff\n",
    "\n",
    "    # With all this done, we can now begin to ask interesting questions\n",
    "    # of the data\n",
    "    #\n",
    "    # For example, let's see who the top 10 donors are.\n",
    "\n",
    "    locale.setlocale(locale.LC_ALL, 'en_CA.UTF-8')  # for pretty printing numbers\n",
    "    \n",
    "    utils.athena_start_query(\"DROP TABLE IF EXISTS e_map\")\n",
    "    q = '''\n",
    "    CREATE TABLE e_map as \n",
    "        SELECT COALESCE(canon_id, entity_map.donor_id) AS canon_id, entity_map.donor_id \n",
    "        FROM entity_map \n",
    "            RIGHT JOIN donors USING(donor_id)\n",
    "    '''\n",
    "    \n",
    "    utils.athena_start_query(q)\n",
    "    q ='''\n",
    "    SELECT array_join(filter(array[donors.first_name, donors.last_name], x-> x IS NOT NULL), ' ') AS name,   \n",
    "        donation_totals.totals AS totals \n",
    "    FROM donors INNER JOIN \n",
    "        (SELECT canon_id, SUM(cast (amount as double)) AS totals \n",
    "        FROM contributions INNER JOIN e_map \n",
    "        USING (donor_id) \n",
    "        GROUP BY (canon_id) \n",
    "        ORDER BY totals \n",
    "        DESC LIMIT 10) \n",
    "        AS donation_totals \n",
    "    ON donors.donor_id = donation_totals.canon_id\n",
    "    ORDER BY totals DESC\n",
    "    '''\n",
    "    cur_dict = as_pandas(q).to_dict('records')\n",
    "\n",
    "    print(\"Top Donors (deduped)\")\n",
    "    for row in cur_dict:\n",
    "        row['totals'] = locale.currency(row['totals'], grouping=True)\n",
    "        print('%(totals)20s: %(name)s' % row)\n",
    "\n",
    "    # Compare this to what we would have gotten if we hadn't done any\n",
    "    # deduplication\n",
    "\n",
    "    q = '''\n",
    "    with donorscontributions as(\n",
    "\n",
    "        SELECT donors.donor_id, \n",
    "            array_join(filter(array[donors.first_name, donors.last_name], x-> x IS NOT NULL), ' ') AS name,\n",
    "            cast(contributions.amount as double) as amount\n",
    "        FROM donors INNER JOIN contributions \n",
    "            USING (donor_id) \n",
    "    )\n",
    "    SELECT name, sum(amount) AS totals  \n",
    "    FROM donorscontributions\n",
    "    GROUP BY donor_id, name\n",
    "    ORDER BY totals DESC \n",
    "    LIMIT 10\n",
    "    '''\n",
    "\n",
    "    cur_dict = as_pandas(q).to_dict('records')\n",
    "\n",
    "    print(\"Top Donors (raw)\")\n",
    "    for row in cur_dict:\n",
    "        row['totals'] = locale.currency(row['totals'], grouping=True)\n",
    "        print('%(totals)20s: %(name)s' % row)\n",
    "\n",
    "    # Close our database connection\n",
    "#     read_con.close()\n",
    "#     write_con.close()\n",
    "\n",
    "    print('ran in', time.time() - start_time, 'seconds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NbConvertApp] Converting notebook athena_example.ipynb to script\n",
      "[NbConvertApp] Writing 11731 bytes to ../athena_example/athena_example.py\n"
     ]
    }
   ],
   "source": [
    "!jupyter nbconvert --to script athena_example.ipynb --output-dir=../athena_example/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
