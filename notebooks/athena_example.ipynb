{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Note: \n",
    "Looks good, but check the sanity check notebook to makesure everything is correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: dedupe in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (2.0.6)\n",
      "Requirement already satisfied: categorical-distance>=1.9 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from dedupe) (1.9)\n",
      "Requirement already satisfied: dedupe-variable-datetime in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from dedupe) (0.1.5)\n",
      "Requirement already satisfied: affinegap>=1.3 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from dedupe) (1.11)\n",
      "Requirement already satisfied: highered>=0.2.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from dedupe) (0.2.1)\n",
      "Requirement already satisfied: typing-extensions in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from dedupe) (3.7.4.3)\n",
      "Requirement already satisfied: simplecosine>=1.2 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from dedupe) (1.2)\n",
      "Requirement already satisfied: doublemetaphone in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from dedupe) (0.1)\n",
      "Requirement already satisfied: fastcluster in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from dedupe) (1.1.26)\n",
      "Requirement already satisfied: rlr>=2.4.3 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from dedupe) (2.4.5)\n",
      "Requirement already satisfied: haversine>=0.4.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from dedupe) (2.3.0)\n",
      "Requirement already satisfied: BTrees>=4.1.4 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from dedupe) (4.7.2)\n",
      "Requirement already satisfied: numpy>=1.13 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from dedupe) (1.18.1)\n",
      "Requirement already satisfied: zope.index in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from dedupe) (5.0.0)\n",
      "Requirement already satisfied: dedupe-hcluster in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from dedupe) (0.3.8)\n",
      "Requirement already satisfied: Levenshtein-search in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from dedupe) (1.4.5)\n",
      "Requirement already satisfied: future in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from dedupe-variable-datetime->dedupe) (0.18.2)\n",
      "Requirement already satisfied: datetime-distance in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from dedupe-variable-datetime->dedupe) (0.1.3)\n",
      "Requirement already satisfied: pyhacrf-datamade>=0.2.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from highered>=0.2.0->dedupe) (0.2.5)\n",
      "Requirement already satisfied: pylbfgs in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from rlr>=2.4.3->dedupe) (0.2.0.13)\n",
      "Requirement already satisfied: zope.interface in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from BTrees>=4.1.4->dedupe) (5.1.2)\n",
      "Requirement already satisfied: persistent>=4.1.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from BTrees>=4.1.4->dedupe) (4.6.4)\n",
      "Requirement already satisfied: six in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from zope.index->dedupe) (1.14.0)\n",
      "Requirement already satisfied: setuptools in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from zope.index->dedupe) (45.2.0.post20200210)\n",
      "Requirement already satisfied: python-dateutil>=2.6.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from datetime-distance->dedupe-variable-datetime->dedupe) (2.8.1)\n",
      "Requirement already satisfied: cffi; platform_python_implementation == \"CPython\" in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from persistent>=4.1.0->BTrees>=4.1.4->dedupe) (1.14.0)\n",
      "Requirement already satisfied: pycparser in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from cffi; platform_python_implementation == \"CPython\"->persistent>=4.1.0->BTrees>=4.1.4->dedupe) (2.19)\n",
      "\u001b[33mWARNING: You are using pip version 20.0.2; however, version 20.2.4 is available.\n",
      "You should consider upgrading via the '/home/ec2-user/anaconda3/envs/python3/bin/python -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install dedupe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This is an example of working with very large data. There are about\n",
    "700,000 unduplicated donors in this database of Illinois political\n",
    "campaign contributions.\n",
    "\n",
    "With such a large set of input data, we cannot store all the comparisons\n",
    "we need to make in memory. Instead, we will read the pairs on demand\n",
    "from the Athena database.\n",
    "\n",
    "__Note:__ You will need to run `python mysql_init_db.py`\n",
    "before running this script. See the annotates source for\n",
    "[mysql_init_db.py](mysql_init_db.html)\n",
    "\n",
    "For smaller datasets (<10,000), see our\n",
    "[csv_example](csv_example.html)\n",
    "\"\"\"\n",
    "\n",
    "# There is a little bit difference between the result \n",
    "# of this module and the mysql one. The reason is due to\n",
    "# Some special (and mostly erroneous) characters, such as \\a .. \n",
    "# Which are dealt with differently by mysql and athena/panda\n",
    "\n",
    "import sys\n",
    "import os\n",
    "import itertools\n",
    "import time\n",
    "import logging\n",
    "import optparse\n",
    "import locale\n",
    "import json\n",
    "from io import StringIO\n",
    "import csv\n",
    "import pandas as pd\n",
    "\n",
    "import boto3\n",
    "import dedupe\n",
    "import dedupe.backport\n",
    "sys.path.insert(0, '../athena_example/')\n",
    "import config\n",
    "sys.path.insert(0, '../athena_example/')\n",
    "import athenautils\n",
    "\n",
    "def as_pandas(query, **kwrgs):\n",
    "    df = athenautils.athena_to_panda(query, escapechar=None, keep_default_na=False, na_values=[''], **kwrgs)\n",
    "    return df.where(pd.notnull(df), None)\n",
    "\n",
    "def record_pairs(result_set):\n",
    "    for i, row in enumerate(result_set):\n",
    "        a_record_id, a_record, b_record_id, b_record = row\n",
    "        record_a = (a_record_id, json.loads(a_record))\n",
    "        record_b = (b_record_id, json.loads(b_record))\n",
    "\n",
    "        yield record_a, record_b\n",
    "\n",
    "        if i % 10000 == 0:\n",
    "            print(i)\n",
    "\n",
    "\n",
    "def cluster_ids(clustered_dupes):\n",
    "\n",
    "    for cluster, scores in clustered_dupes:\n",
    "        cluster_id = cluster[0]\n",
    "        for donor_id, score in zip(cluster, scores):\n",
    "            yield donor_id, cluster_id, score\n",
    "\n",
    "\n",
    "# if __name__ == '__main__':\n",
    "if True:\n",
    "\n",
    "    # ## Logging\n",
    "\n",
    "    # Dedupe uses Python logging to show or suppress verbose output. Added\n",
    "    # for convenience.  To enable verbose output, run `python\n",
    "    # examples/mysql_example/mysql_example.py -v`\n",
    "    \n",
    "#     optp = optparse.OptionParser()\n",
    "#     optp.add_option('-v', '--verbose', dest='verbose', action='count',\n",
    "#                     help='Increase verbosity (specify multiple times for more)'\n",
    "#                     )\n",
    "#     (opts, args) = optp.parse_args()\n",
    "    log_level = logging.WARNING\n",
    "#     if opts.verbose:\n",
    "#         if opts.verbose == 1:\n",
    "#             log_level = logging.INFO\n",
    "#         elif opts.verbose >= 2:\n",
    "#             log_level = logging.DEBUG\n",
    "\n",
    "\n",
    "    logging.getLogger().setLevel(log_level)\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "    settings_file = 'mysql_example_settings'\n",
    "    training_file = 'mysql_example_training.json'\n",
    "\n",
    "    start_time = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reading from  mysql_example_settings\n"
     ]
    }
   ],
   "source": [
    "    # We'll be using variations on this following select statement to pull\n",
    "    # in campaign donor info.\n",
    "    #\n",
    "    # We did a fair amount of preprocessing of the fields in\n",
    "    # `mysql_init_db.py`    \n",
    "    DONOR_SELECT = \"\"\"SELECT donor_id, city, name, zip, state, address\n",
    "                      from as_processed_donors\"\"\"\n",
    "\n",
    "    # ## Training\n",
    "\n",
    "    if os.path.exists(settings_file):\n",
    "        print('reading from ', settings_file)\n",
    "        with open(settings_file, 'rb') as sf:\n",
    "            deduper = dedupe.StaticDedupe(sf, num_cores=4)\n",
    "    else:\n",
    "        # Define the fields dedupe will pay attention to\n",
    "        #\n",
    "        # The address, city, and zip fields are often missing, so we'll\n",
    "        # tell dedupe that, and we'll learn a model that take that into\n",
    "        # account\n",
    "        fields = [{'field': 'name', 'type': 'String'},\n",
    "                  {'field': 'address', 'type': 'String',\n",
    "                   'has missing': True},\n",
    "                  {'field': 'city', 'type': 'ShortString', 'has missing': True},\n",
    "                  {'field': 'state', 'type': 'ShortString', 'has missing': True},\n",
    "                  {'field': 'zip', 'type': 'ShortString', 'has missing': True},\n",
    "                  ]\n",
    "\n",
    "        # Create a new deduper object and pass our data model to it.\n",
    "        deduper = dedupe.Dedupe(fields, num_cores=4)\n",
    "\n",
    "        # We will sample pairs from the entire donor table for training\n",
    "        cur = cur_execute(DONOR_SELECT)\n",
    "        temp_d = {i: row for i, row in enumerate(cur)}\n",
    "            \n",
    "\n",
    "        # If we have training data saved from a previous run of dedupe,\n",
    "        # look for it an load it in.\n",
    "        #\n",
    "        # __Note:__ if you want to train from\n",
    "        # scratch, delete the training_file\n",
    "        if os.path.exists(training_file):\n",
    "            print('reading labeled examples from ', training_file)\n",
    "            with open(training_file) as tf:\n",
    "                deduper.prepare_training(temp_d, training_file=tf)\n",
    "        else:\n",
    "            deduper.prepare_training(temp_d)\n",
    "\n",
    "        del temp_d\n",
    "\n",
    "        # ## Active learning\n",
    "\n",
    "        print('starting active labeling...')\n",
    "        # Starts the training loop. Dedupe will find the next pair of records\n",
    "        # it is least certain about and ask you to label them as duplicates\n",
    "        # or not.\n",
    "\n",
    "        # use 'y', 'n' and 'u' keys to flag duplicates\n",
    "        # press 'f' when you are finished\n",
    "        dedupe.convenience.console_label(deduper)\n",
    "        # When finished, save our labeled, training pairs to disk\n",
    "        with open(training_file, 'w') as tf:\n",
    "            deduper.write_training(tf)\n",
    "\n",
    "        # Notice our the argument here\n",
    "        #\n",
    "        # `recall` is the proportion of true dupes pairs that the learned\n",
    "        # rules must cover. You may want to reduce this if your are making\n",
    "        # too many blocks and too many comparisons.\n",
    "        deduper.train(recall=0.90)\n",
    "\n",
    "        with open(settings_file, 'wb') as sf:\n",
    "            deduper.write_settings(sf)\n",
    "\n",
    "        # We can now remove some of the memory hobbing objects we used\n",
    "        # for training\n",
    "        deduper.cleanup_training()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "blocking...\n",
      "creating as_blocking_map database\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'5651b314-d20b-4404-aa8d-30df70804e0e'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "    # ## Blocking\n",
    "\n",
    "    print('blocking...')\n",
    "\n",
    "    # To run blocking on such a large set of data, we create a separate table\n",
    "    # that contains blocking keys and record ids\n",
    "    print('creating as_blocking_map database')\n",
    "    athenautils.athena_start_query(\"DROP TABLE IF EXISTS as_blocking_map\", database=config.DATABASE)\n",
    "\n",
    "    q=\"\"\"\n",
    "    CREATE EXTERNAL TABLE as_blocking_map     \n",
    "        (block_key VARCHAR(200), donor_id INTEGER)\n",
    "    ROW FORMAT DELIMITED\n",
    "      FIELDS TERMINATED BY '\\t'\n",
    "      LINES TERMINATED BY '\\n'  \n",
    "    LOCATION\n",
    "        's3://{}/{}' \n",
    "    TBLPROPERTIES (\n",
    "        'classification'='csv', \n",
    "        --'skip.header.line.count'='1',  \n",
    "        'serialization.null.format'='')\n",
    "    \"\"\".format(config.DATABASE_BUCKET, config.DATABASE_ROOT_KEY+'as_blocking_map') \n",
    "    athenautils.athena_start_query(q, database=config.DATABASE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "creating inverted index\n"
     ]
    }
   ],
   "source": [
    "    # If dedupe learned a Index Predicate, we have to take a pass\n",
    "    # through the data and create indices.\n",
    "    print('creating inverted index')\n",
    "\n",
    "    # Armin: \n",
    "    # This never runs, index_fields is empty, possible bug?\n",
    "    for field in deduper.fingerprinter.index_fields:\n",
    "        q = \"\"\"\n",
    "        SELECT DISTINCT {field} FROM as_processed_donors\n",
    "        WHERE {field} IS NOT NULL\n",
    "        \"\"\".format(field=field)\n",
    "        cur = cur_execute(q)\n",
    "        field_data = (row[field] for row in cur)\n",
    "        deduper.fingerprinter.index(field_data, field)\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "writing blocking map\n"
     ]
    }
   ],
   "source": [
    "    # Now we are ready to write our blocking map table by creating a\n",
    "    # generator that yields unique `(block_key, donor_id)` tuples.\n",
    "    print('writing blocking map')\n",
    "    \n",
    "    read_cur  = athenautils.cursor_execute(DONOR_SELECT, database=config.DATABASE)\n",
    "    full_data = ((row['donor_id'], row) for row in read_cur)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s3://ria-temp/as_dedupe/as_blocking_map/blocking_0.csv\n",
      "s3://ria-temp/as_dedupe/as_blocking_map/blocking_1.csv\n",
      "s3://ria-temp/as_dedupe/as_blocking_map/blocking_2.csv\n",
      "s3://ria-temp/as_dedupe/as_blocking_map/blocking_3.csv\n",
      "s3://ria-temp/as_dedupe/as_blocking_map/blocking_4.csv\n",
      "s3://ria-temp/as_dedupe/as_blocking_map/blocking_5.csv\n",
      "s3://ria-temp/as_dedupe/as_blocking_map/blocking_6.csv\n",
      "s3://ria-temp/as_dedupe/as_blocking_map/blocking_7.csv\n",
      "s3://ria-temp/as_dedupe/as_blocking_map/blocking_8.csv\n",
      "s3://ria-temp/as_dedupe/as_blocking_map/blocking_9.csv\n",
      "s3://ria-temp/as_dedupe/as_blocking_map/blocking_10.csv\n",
      "s3://ria-temp/as_dedupe/as_blocking_map/blocking_11.csv\n",
      "s3://ria-temp/as_dedupe/as_blocking_map/blocking_12.csv\n",
      "s3://ria-temp/as_dedupe/as_blocking_map/blocking_13.csv\n",
      "s3://ria-temp/as_dedupe/as_blocking_map/blocking_14.csv\n",
      "s3://ria-temp/as_dedupe/as_blocking_map/blocking_15.csv\n",
      "s3://ria-temp/as_dedupe/as_blocking_map/blocking_16.csv\n",
      "s3://ria-temp/as_dedupe/as_blocking_map/blocking_17.csv\n",
      "s3://ria-temp/as_dedupe/as_blocking_map/blocking_18.csv\n",
      "s3://ria-temp/as_dedupe/as_blocking_map/blocking_19.csv\n",
      "s3://ria-temp/as_dedupe/as_blocking_map/blocking_20.csv\n",
      "s3://ria-temp/as_dedupe/as_blocking_map/blocking_21.csv\n",
      "s3://ria-temp/as_dedupe/as_blocking_map/blocking_22.csv\n",
      "s3://ria-temp/as_dedupe/as_blocking_map/blocking_23.csv\n",
      "s3://ria-temp/as_dedupe/as_blocking_map/blocking_24.csv\n",
      "s3://ria-temp/as_dedupe/as_blocking_map/blocking_25.csv\n",
      "s3://ria-temp/as_dedupe/as_blocking_map/blocking_26.csv\n",
      "s3://ria-temp/as_dedupe/as_blocking_map/blocking_27.csv\n",
      "s3://ria-temp/as_dedupe/as_blocking_map/blocking_28.csv\n",
      "s3://ria-temp/as_dedupe/as_blocking_map/blocking_29.csv\n",
      "s3://ria-temp/as_dedupe/as_blocking_map/blocking_30.csv\n",
      "s3://ria-temp/as_dedupe/as_blocking_map/blocking_31.csv\n",
      "s3://ria-temp/as_dedupe/as_blocking_map/blocking_32.csv\n",
      "s3://ria-temp/as_dedupe/as_blocking_map/blocking_33.csv\n",
      "s3://ria-temp/as_dedupe/as_blocking_map/blocking_34.csv\n",
      "s3://ria-temp/as_dedupe/as_blocking_map/blocking_35.csv\n",
      "s3://ria-temp/as_dedupe/as_blocking_map/blocking_36.csv\n",
      "s3://ria-temp/as_dedupe/as_blocking_map/blocking_37.csv\n",
      "s3://ria-temp/as_dedupe/as_blocking_map/blocking_38.csv\n",
      "s3://ria-temp/as_dedupe/as_blocking_map/blocking_39.csv\n",
      "s3://ria-temp/as_dedupe/as_blocking_map/blocking_40.csv\n",
      "s3://ria-temp/as_dedupe/as_blocking_map/blocking_41.csv\n",
      "s3://ria-temp/as_dedupe/as_blocking_map/blocking_42.csv\n",
      "s3://ria-temp/as_dedupe/as_blocking_map/blocking_43.csv\n",
      "s3://ria-temp/as_dedupe/as_blocking_map/blocking_44.csv\n",
      "s3://ria-temp/as_dedupe/as_blocking_map/blocking_45.csv\n",
      "s3://ria-temp/as_dedupe/as_blocking_map/blocking_46.csv\n",
      "s3://ria-temp/as_dedupe/as_blocking_map/blocking_47.csv\n",
      "s3://ria-temp/as_dedupe/as_blocking_map/blocking_48.csv\n",
      "s3://ria-temp/as_dedupe/as_blocking_map/blocking_49.csv\n",
      "s3://ria-temp/as_dedupe/as_blocking_map/blocking_50.csv\n",
      "s3://ria-temp/as_dedupe/as_blocking_map/blocking_51.csv\n",
      "s3://ria-temp/as_dedupe/as_blocking_map/blocking_52.csv\n",
      "s3://ria-temp/as_dedupe/as_blocking_map/blocking_53.csv\n",
      "s3://ria-temp/as_dedupe/as_blocking_map/blocking_54.csv\n",
      "s3://ria-temp/as_dedupe/as_blocking_map/blocking_55.csv\n",
      "s3://ria-temp/as_dedupe/as_blocking_map/blocking_56.csv\n",
      "s3://ria-temp/as_dedupe/as_blocking_map/blocking_57.csv\n",
      "s3://ria-temp/as_dedupe/as_blocking_map/blocking_58.csv\n",
      "s3://ria-temp/as_dedupe/as_blocking_map/blocking_59.csv\n",
      "s3://ria-temp/as_dedupe/as_blocking_map/blocking_60.csv\n",
      "s3://ria-temp/as_dedupe/as_blocking_map/blocking_61.csv\n",
      "s3://ria-temp/as_dedupe/as_blocking_map/blocking_62.csv\n",
      "s3://ria-temp/as_dedupe/as_blocking_map/blocking_63.csv\n",
      "s3://ria-temp/as_dedupe/as_blocking_map/blocking_64.csv\n",
      "s3://ria-temp/as_dedupe/as_blocking_map/blocking_65.csv\n",
      "s3://ria-temp/as_dedupe/as_blocking_map/blocking_66.csv\n",
      "s3://ria-temp/as_dedupe/as_blocking_map/blocking_67.csv\n",
      "s3://ria-temp/as_dedupe/as_blocking_map/blocking_68.csv\n",
      "s3://ria-temp/as_dedupe/as_blocking_map/blocking_69.csv\n",
      "s3://ria-temp/as_dedupe/as_blocking_map/blocking_70.csv\n",
      "s3://ria-temp/as_dedupe/as_blocking_map/blocking_71.csv\n",
      "s3://ria-temp/as_dedupe/as_blocking_map/blocking_72.csv\n",
      "s3://ria-temp/as_dedupe/as_blocking_map/blocking_73.csv\n",
      "s3://ria-temp/as_dedupe/as_blocking_map/blocking_74.csv\n",
      "s3://ria-temp/as_dedupe/as_blocking_map/blocking_75.csv\n",
      "s3://ria-temp/as_dedupe/as_blocking_map/blocking_76.csv\n",
      "s3://ria-temp/as_dedupe/as_blocking_map/blocking_77.csv\n",
      "s3://ria-temp/as_dedupe/as_blocking_map/blocking_78.csv\n",
      "s3://ria-temp/as_dedupe/as_blocking_map/blocking_79.csv\n",
      "s3://ria-temp/as_dedupe/as_blocking_map/blocking_80.csv\n",
      "s3://ria-temp/as_dedupe/as_blocking_map/blocking_81.csv\n",
      "s3://ria-temp/as_dedupe/as_blocking_map/blocking_82.csv\n",
      "s3://ria-temp/as_dedupe/as_blocking_map/blocking_83.csv\n",
      "s3://ria-temp/as_dedupe/as_blocking_map/blocking_84.csv\n",
      "s3://ria-temp/as_dedupe/as_blocking_map/blocking_85.csv\n",
      "s3://ria-temp/as_dedupe/as_blocking_map/blocking_86.csv\n",
      "s3://ria-temp/as_dedupe/as_blocking_map/blocking_87.csv\n",
      "s3://ria-temp/as_dedupe/as_blocking_map/blocking_88.csv\n",
      "s3://ria-temp/as_dedupe/as_blocking_map/blocking_89.csv\n"
     ]
    }
   ],
   "source": [
    "    b_data = deduper.fingerprinter(full_data)\n",
    "    athenautils.write_many(b_data, \n",
    "                           filename=os.path.join(\"s3://\", config.DATABASE_BUCKET, config.DATABASE_ROOT_KEY, 'as_blocking_map/blocking.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "    # select unique pairs to compare\n",
    "    q=\"\"\"\n",
    "        SELECT a.donor_id,\n",
    "            json_format(CAST (MAP(ARRAY['city', 'name', 'zip', 'state', 'address'],\n",
    "                                  ARRAY[ a.city, a.name, a.zip, a.state, a.address])\n",
    "                        AS JSON)),\n",
    "            b.donor_id,\n",
    "            json_format(CAST (MAP(ARRAY['city', 'name', 'zip', 'state', 'address'], \n",
    "                      ARRAY[ b.city, b.name, b.zip, b.state, b.address])\n",
    "                  AS JSON))\n",
    "        FROM (SELECT DISTINCT l.donor_id as east, r.donor_id as west\n",
    "             from as_blocking_map as l\n",
    "             INNER JOIN as_blocking_map as r\n",
    "             using (block_key)\n",
    "             where l.donor_id < r.donor_id) ids\n",
    "        INNER JOIN as_processed_donors a on ids.east=a.donor_id\n",
    "        INNER JOIN as_processed_donors b on ids.west=b.donor_id\n",
    "       \"\"\"\n",
    "    read_cur = athenautils.cursor_execute(q, cursortype='tuple', database=config.DATABASE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "clustering...\n",
      "0\n",
      "10000\n",
      "20000\n",
      "30000\n",
      "40000\n",
      "50000\n",
      "60000\n",
      "70000\n",
      "80000\n",
      "90000\n",
      "100000\n",
      "110000\n",
      "120000\n",
      "130000\n",
      "140000\n",
      "150000\n",
      "160000\n",
      "170000\n",
      "180000\n",
      "190000\n",
      "200000\n",
      "210000\n",
      "220000\n",
      "230000\n",
      "240000\n",
      "250000\n",
      "260000\n",
      "270000\n",
      "280000\n",
      "290000\n",
      "300000\n",
      "310000\n",
      "320000\n",
      "330000\n",
      "340000\n",
      "350000\n",
      "360000\n",
      "370000\n",
      "380000\n",
      "390000\n",
      "400000\n",
      "410000\n",
      "420000\n",
      "430000\n",
      "440000\n",
      "450000\n",
      "460000\n",
      "470000\n",
      "480000\n",
      "490000\n",
      "500000\n",
      "510000\n",
      "520000\n",
      "530000\n",
      "540000\n",
      "550000\n",
      "560000\n",
      "570000\n",
      "580000\n",
      "590000\n",
      "600000\n",
      "610000\n",
      "620000\n",
      "630000\n",
      "640000\n",
      "650000\n",
      "660000\n",
      "670000\n",
      "680000\n",
      "690000\n",
      "700000\n",
      "710000\n",
      "720000\n",
      "730000\n",
      "740000\n",
      "750000\n",
      "760000\n",
      "770000\n",
      "780000\n",
      "790000\n",
      "800000\n",
      "810000\n",
      "820000\n",
      "830000\n",
      "840000\n",
      "850000\n",
      "860000\n",
      "870000\n",
      "880000\n",
      "890000\n",
      "900000\n",
      "910000\n",
      "920000\n",
      "930000\n",
      "940000\n",
      "950000\n",
      "960000\n",
      "970000\n",
      "980000\n",
      "990000\n",
      "1000000\n",
      "1010000\n",
      "1020000\n",
      "1030000\n",
      "1040000\n",
      "1050000\n",
      "1060000\n",
      "1070000\n",
      "1080000\n",
      "1090000\n",
      "1100000\n",
      "1110000\n",
      "1120000\n",
      "1130000\n",
      "1140000\n",
      "1150000\n",
      "1160000\n",
      "1170000\n",
      "1180000\n",
      "1190000\n",
      "1200000\n",
      "1210000\n",
      "1220000\n",
      "1230000\n",
      "1240000\n",
      "1250000\n",
      "1260000\n",
      "1270000\n",
      "1280000\n",
      "1290000\n",
      "1300000\n",
      "1310000\n",
      "1320000\n",
      "1330000\n",
      "1340000\n",
      "1350000\n",
      "1360000\n",
      "1370000\n",
      "1380000\n",
      "1390000\n",
      "1400000\n",
      "1410000\n",
      "1420000\n",
      "1430000\n",
      "1440000\n",
      "1450000\n",
      "1460000\n",
      "1470000\n",
      "1480000\n",
      "1490000\n",
      "1500000\n",
      "1510000\n",
      "1520000\n",
      "1530000\n",
      "1540000\n",
      "1550000\n",
      "1560000\n",
      "1570000\n",
      "1580000\n",
      "1590000\n",
      "1600000\n",
      "1610000\n",
      "1620000\n",
      "1630000\n",
      "1640000\n",
      "1650000\n",
      "1660000\n",
      "1670000\n",
      "1680000\n",
      "1690000\n",
      "1700000\n",
      "1710000\n",
      "1720000\n",
      "1730000\n",
      "1740000\n",
      "1750000\n",
      "1760000\n",
      "1770000\n",
      "1780000\n",
      "1790000\n",
      "1800000\n",
      "1810000\n",
      "1820000\n",
      "1830000\n",
      "1840000\n",
      "1850000\n",
      "1860000\n",
      "1870000\n",
      "1880000\n",
      "1890000\n",
      "1900000\n",
      "1910000\n",
      "1920000\n",
      "1930000\n",
      "1940000\n",
      "1950000\n",
      "1960000\n",
      "1970000\n",
      "1980000\n",
      "1990000\n",
      "2000000\n",
      "2010000\n",
      "2020000\n",
      "2030000\n",
      "2040000\n",
      "2050000\n",
      "2060000\n",
      "2070000\n",
      "2080000\n",
      "2090000\n",
      "2100000\n",
      "2110000\n",
      "2120000\n",
      "2130000\n",
      "2140000\n",
      "2150000\n",
      "2160000\n",
      "2170000\n",
      "2180000\n",
      "2190000\n",
      "2200000\n",
      "2210000\n",
      "2220000\n",
      "2230000\n",
      "2240000\n",
      "2250000\n",
      "2260000\n",
      "2270000\n",
      "2280000\n",
      "2290000\n",
      "2300000\n",
      "2310000\n",
      "2320000\n",
      "2330000\n",
      "2340000\n",
      "2350000\n",
      "2360000\n",
      "2370000\n",
      "2380000\n",
      "2390000\n",
      "2400000\n",
      "2410000\n",
      "2420000\n",
      "2430000\n",
      "2440000\n",
      "2450000\n",
      "2460000\n",
      "2470000\n",
      "2480000\n",
      "2490000\n",
      "2500000\n",
      "2510000\n",
      "2520000\n",
      "2530000\n",
      "2540000\n",
      "2550000\n",
      "2560000\n",
      "2570000\n",
      "2580000\n",
      "2590000\n",
      "2600000\n",
      "2610000\n",
      "2620000\n",
      "2630000\n",
      "2640000\n",
      "2650000\n",
      "2660000\n",
      "2670000\n",
      "2680000\n",
      "2690000\n",
      "2700000\n",
      "2710000\n",
      "2720000\n",
      "2730000\n",
      "2740000\n",
      "2750000\n",
      "2760000\n",
      "2770000\n",
      "2780000\n",
      "2790000\n",
      "2800000\n",
      "2810000\n",
      "2820000\n",
      "2830000\n",
      "2840000\n",
      "2850000\n",
      "2860000\n",
      "2870000\n",
      "2880000\n",
      "2890000\n",
      "2900000\n",
      "2910000\n",
      "2920000\n",
      "2930000\n",
      "2940000\n",
      "2950000\n",
      "2960000\n",
      "2970000\n",
      "2980000\n",
      "2990000\n",
      "3000000\n",
      "3010000\n",
      "3020000\n",
      "3030000\n",
      "3040000\n",
      "3050000\n",
      "3060000\n",
      "3070000\n",
      "3080000\n",
      "3090000\n",
      "3100000\n",
      "3110000\n",
      "3120000\n",
      "3130000\n",
      "3140000\n",
      "3150000\n",
      "3160000\n",
      "3170000\n",
      "3180000\n",
      "3190000\n",
      "3200000\n",
      "3210000\n",
      "3220000\n",
      "3230000\n",
      "3240000\n",
      "3250000\n",
      "3260000\n",
      "3270000\n",
      "3280000\n",
      "3290000\n",
      "3300000\n",
      "3310000\n",
      "3320000\n",
      "3330000\n",
      "3340000\n",
      "3350000\n",
      "3360000\n",
      "3370000\n",
      "3380000\n",
      "3390000\n",
      "3400000\n",
      "3410000\n",
      "3420000\n",
      "3430000\n",
      "3440000\n",
      "3450000\n",
      "3460000\n",
      "3470000\n",
      "3480000\n",
      "3490000\n",
      "3500000\n",
      "3510000\n",
      "3520000\n",
      "3530000\n",
      "3540000\n",
      "3550000\n",
      "3560000\n",
      "3570000\n",
      "3580000\n",
      "3590000\n",
      "3600000\n",
      "3610000\n",
      "3620000\n",
      "3630000\n",
      "3640000\n",
      "3650000\n",
      "3660000\n",
      "3670000\n",
      "3680000\n",
      "3690000\n",
      "3700000\n",
      "3710000\n",
      "3720000\n",
      "3730000\n",
      "3740000\n",
      "3750000\n",
      "3760000\n",
      "3770000\n",
      "3780000\n",
      "3790000\n",
      "3800000\n",
      "3810000\n",
      "3820000\n",
      "3830000\n",
      "3840000\n",
      "3850000\n",
      "3860000\n",
      "3870000\n",
      "3880000\n",
      "3890000\n",
      "3900000\n",
      "3910000\n",
      "3920000\n",
      "3930000\n",
      "3940000\n",
      "3950000\n",
      "3960000\n",
      "3970000\n",
      "3980000\n",
      "3990000\n",
      "4000000\n",
      "4010000\n",
      "4020000\n",
      "4030000\n",
      "4040000\n",
      "4050000\n",
      "4060000\n",
      "4070000\n",
      "4080000\n",
      "4090000\n",
      "4100000\n",
      "4110000\n",
      "4120000\n",
      "4130000\n",
      "4140000\n",
      "4150000\n",
      "4160000\n",
      "4170000\n",
      "4180000\n",
      "4190000\n",
      "4200000\n",
      "4210000\n",
      "4220000\n",
      "4230000\n",
      "4240000\n",
      "4250000\n",
      "4260000\n",
      "4270000\n",
      "4280000\n",
      "4290000\n",
      "4300000\n",
      "4310000\n",
      "4320000\n",
      "4330000\n",
      "4340000\n",
      "4350000\n",
      "4360000\n",
      "4370000\n",
      "4380000\n",
      "4390000\n",
      "4400000\n",
      "4410000\n",
      "4420000\n",
      "4430000\n",
      "4440000\n",
      "4450000\n",
      "4460000\n",
      "4470000\n",
      "4480000\n",
      "4490000\n",
      "4500000\n",
      "4510000\n",
      "4520000\n",
      "4530000\n",
      "4540000\n",
      "4550000\n",
      "4560000\n",
      "4570000\n",
      "4580000\n",
      "4590000\n",
      "4600000\n",
      "4610000\n",
      "4620000\n",
      "4630000\n",
      "4640000\n",
      "4650000\n",
      "4660000\n",
      "4670000\n",
      "4680000\n",
      "4690000\n",
      "4700000\n",
      "4710000\n",
      "4720000\n",
      "4730000\n",
      "4740000\n",
      "4750000\n",
      "4760000\n",
      "4770000\n",
      "4780000\n",
      "4790000\n",
      "4800000\n",
      "4810000\n",
      "4820000\n",
      "4830000\n",
      "4840000\n",
      "4850000\n",
      "4860000\n",
      "4870000\n",
      "4880000\n",
      "4890000\n",
      "4900000\n",
      "4910000\n",
      "4920000\n",
      "4930000\n",
      "4940000\n",
      "4950000\n",
      "4960000\n",
      "4970000\n",
      "4980000\n",
      "4990000\n",
      "5000000\n",
      "5010000\n",
      "5020000\n",
      "5030000\n",
      "5040000\n",
      "5050000\n",
      "5060000\n",
      "5070000\n",
      "5080000\n",
      "5090000\n",
      "5100000\n",
      "5110000\n",
      "5120000\n"
     ]
    }
   ],
   "source": [
    "    # ## Clustering\n",
    "\n",
    "    print('clustering...')\n",
    "    clustered_dupes = deduper.cluster(deduper.score(record_pairs(read_cur)),\n",
    "                                      threshold=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "creating as_entity_map database\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:dedupe.clustering:A component contained 158378 elements. Components larger than 30000 are re-filtered. The threshold for this filtering is 3.048800940974382e-18\n",
      "WARNING:dedupe.clustering:A component contained 158378 elements. Components larger than 30000 are re-filtered. The threshold for this filtering is 1.1038451096258602e-17\n",
      "WARNING:dedupe.clustering:A component contained 158378 elements. Components larger than 30000 are re-filtered. The threshold for this filtering is 4.921472802362437e-17\n",
      "WARNING:dedupe.clustering:A component contained 158376 elements. Components larger than 30000 are re-filtered. The threshold for this filtering is 1.3509749469939632e-16\n",
      "WARNING:dedupe.clustering:A component contained 158376 elements. Components larger than 30000 are re-filtered. The threshold for this filtering is 3.753198598980096e-16\n",
      "WARNING:dedupe.clustering:A component contained 158376 elements. Components larger than 30000 are re-filtered. The threshold for this filtering is 1.0403512617910142e-15\n",
      "WARNING:dedupe.clustering:A component contained 158376 elements. Components larger than 30000 are re-filtered. The threshold for this filtering is 2.894390795098561e-15\n",
      "WARNING:dedupe.clustering:A component contained 158376 elements. Components larger than 30000 are re-filtered. The threshold for this filtering is 8.13221083415663e-15\n",
      "WARNING:dedupe.clustering:A component contained 158376 elements. Components larger than 30000 are re-filtered. The threshold for this filtering is 2.2319167959587083e-14\n",
      "WARNING:dedupe.clustering:A component contained 158375 elements. Components larger than 30000 are re-filtered. The threshold for this filtering is 6.068761194789993e-14\n",
      "WARNING:dedupe.clustering:A component contained 158372 elements. Components larger than 30000 are re-filtered. The threshold for this filtering is 1.6520029686587212e-13\n",
      "WARNING:dedupe.clustering:A component contained 158364 elements. Components larger than 30000 are re-filtered. The threshold for this filtering is 4.5125040046676256e-13\n",
      "WARNING:dedupe.clustering:A component contained 158352 elements. Components larger than 30000 are re-filtered. The threshold for this filtering is 1.2269463311618112e-12\n",
      "WARNING:dedupe.clustering:A component contained 158314 elements. Components larger than 30000 are re-filtered. The threshold for this filtering is 3.3356439660236965e-12\n",
      "WARNING:dedupe.clustering:A component contained 157999 elements. Components larger than 30000 are re-filtered. The threshold for this filtering is 9.069763004960628e-12\n",
      "WARNING:dedupe.clustering:A component contained 157528 elements. Components larger than 30000 are re-filtered. The threshold for this filtering is 2.4668471436592435e-11\n",
      "WARNING:dedupe.clustering:A component contained 157002 elements. Components larger than 30000 are re-filtered. The threshold for this filtering is 6.705726454279488e-11\n",
      "WARNING:dedupe.clustering:A component contained 156034 elements. Components larger than 30000 are re-filtered. The threshold for this filtering is 1.822899310600237e-10\n",
      "WARNING:dedupe.clustering:A component contained 153167 elements. Components larger than 30000 are re-filtered. The threshold for this filtering is 4.955617200886392e-10\n",
      "WARNING:dedupe.clustering:A component contained 150749 elements. Components larger than 30000 are re-filtered. The threshold for this filtering is 1.3472511446191333e-09\n",
      "WARNING:dedupe.clustering:A component contained 148126 elements. Components larger than 30000 are re-filtered. The threshold for this filtering is 3.663451859737113e-09\n",
      "WARNING:dedupe.clustering:A component contained 144445 elements. Components larger than 30000 are re-filtered. The threshold for this filtering is 9.961619074337575e-09\n",
      "WARNING:dedupe.clustering:A component contained 140752 elements. Components larger than 30000 are re-filtered. The threshold for this filtering is 2.7079365851019727e-08\n",
      "WARNING:dedupe.clustering:A component contained 136821 elements. Components larger than 30000 are re-filtered. The threshold for this filtering is 7.361173277021834e-08\n",
      "WARNING:dedupe.clustering:A component contained 132985 elements. Components larger than 30000 are re-filtered. The threshold for this filtering is 2.00129481181664e-07\n",
      "WARNING:dedupe.clustering:A component contained 129188 elements. Components larger than 30000 are re-filtered. The threshold for this filtering is 5.440113266301783e-07\n",
      "WARNING:dedupe.clustering:A component contained 126461 elements. Components larger than 30000 are re-filtered. The threshold for this filtering is 1.4789049802767608e-06\n",
      "WARNING:dedupe.clustering:A component contained 124279 elements. Components larger than 30000 are re-filtered. The threshold for this filtering is 4.020875427015852e-06\n",
      "WARNING:dedupe.clustering:A component contained 121039 elements. Components larger than 30000 are re-filtered. The threshold for this filtering is 1.0930919387732102e-05\n",
      "WARNING:dedupe.clustering:A component contained 117376 elements. Components larger than 30000 are re-filtered. The threshold for this filtering is 2.971327846301476e-05\n",
      "WARNING:dedupe.clustering:A component contained 114455 elements. Components larger than 30000 are re-filtered. The threshold for this filtering is 8.076722851404745e-05\n",
      "WARNING:dedupe.clustering:A component contained 109969 elements. Components larger than 30000 are re-filtered. The threshold for this filtering is 0.0002195200394847895\n",
      "WARNING:dedupe.clustering:A component contained 106867 elements. Components larger than 30000 are re-filtered. The threshold for this filtering is 0.0005965000236037636\n",
      "WARNING:dedupe.clustering:A component contained 101488 elements. Components larger than 30000 are re-filtered. The threshold for this filtering is 0.001619959237855584\n",
      "WARNING:dedupe.clustering:A component contained 94945 elements. Components larger than 30000 are re-filtered. The threshold for this filtering is 0.004391296209574281\n",
      "WARNING:dedupe.clustering:A component contained 89944 elements. Components larger than 30000 are re-filtered. The threshold for this filtering is 0.01184744490841891\n",
      "WARNING:dedupe.clustering:A component contained 85759 elements. Components larger than 30000 are re-filtered. The threshold for this filtering is 0.031562819826922474\n",
      "WARNING:dedupe.clustering:A component contained 79119 elements. Components larger than 30000 are re-filtered. The threshold for this filtering is 0.08138832068049236\n",
      "WARNING:dedupe.clustering:A component contained 73185 elements. Components larger than 30000 are re-filtered. The threshold for this filtering is 0.1940994212134007\n",
      "WARNING:dedupe.clustering:A component contained 67046 elements. Components larger than 30000 are re-filtered. The threshold for this filtering is 0.39565940016357204\n",
      "WARNING:dedupe.clustering:A component contained 57601 elements. Components larger than 30000 are re-filtered. The threshold for this filtering is 0.6402437741869547\n",
      "WARNING:dedupe.clustering:A component contained 36731 elements. Components larger than 30000 are re-filtered. The threshold for this filtering is 0.8286982262391892\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s3://ria-temp/as_dedupe/as_entity_map/as_entity_map_0.csv\n",
      "s3://ria-temp/as_dedupe/as_entity_map/as_entity_map_1.csv\n",
      "s3://ria-temp/as_dedupe/as_entity_map/as_entity_map_2.csv\n",
      "s3://ria-temp/as_dedupe/as_entity_map/as_entity_map_3.csv\n",
      "s3://ria-temp/as_dedupe/as_entity_map/as_entity_map_4.csv\n"
     ]
    }
   ],
   "source": [
    "    athenautils.athena_start_query(\"DROP TABLE IF EXISTS as_entity_map\", database=config.DATABASE)\n",
    "\n",
    "    print('creating as_entity_map database')\n",
    "    q=\"\"\"\n",
    "    CREATE EXTERNAL TABLE as_entity_map     \n",
    "        (donor_id INTEGER, canon_id INTEGER, \n",
    "         cluster_score FLOAT)\n",
    "    ROW FORMAT DELIMITED\n",
    "      FIELDS TERMINATED BY '\\t'\n",
    "      LINES TERMINATED BY '\\n'  \n",
    "    LOCATION\n",
    "        's3://{}/{}' \n",
    "    TBLPROPERTIES (\n",
    "        'classification'='csv', \n",
    "        --'skip.header.line.count'='1',  \n",
    "        'serialization.null.format'='')\n",
    "    \"\"\".format(config.DATABASE_BUCKET, config.DATABASE_ROOT_KEY+'as_entity_map') \n",
    "    athenautils.athena_start_query(q, database=config.DATABASE) \n",
    "\n",
    "    athenautils.write_many(cluster_ids(clustered_dupes),\n",
    "                          filename=os.path.join(\"s3://\", config.DATABASE_BUCKET, config.DATABASE_ROOT_KEY, 'as_entity_map/as_entity_map.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# duplicate sets\n",
      "Top Donors (deduped)\n",
      "      $32,146,134.06: democratic party of illinois\n",
      "      $13,762,181.54: republican state senate campaign committee\n",
      "       $9,590,682.54: republican governors association\n",
      "       $9,040,913.46: madigan michael friends of\n",
      "       $7,949,218.49: seiu healthcare il in pac\n",
      "       $6,435,815.20: chicago teachers union, ift local 1\n",
      "       $6,353,463.90: illinois senate democratic fund (the)\n",
      "       $6,077,259.02: fred eychaner\n",
      "       $6,022,884.47: scott cohen\n",
      "       $5,911,667.89: illinois republican party\n",
      "Top Donors (raw)\n",
      "      $14,319,194.47: democratic party of illinois\n",
      "      $13,020,132.76: democratic party of illinois\n",
      "       $9,027,432.54: republican governors association\n",
      "       $7,897,829.31: rga illinois 2010 pac\n",
      "       $6,675,000.00: madigan michael friends of\n",
      "       $6,008,841.69: scott cohen\n",
      "       $5,570,839.00: ronald gidwitz,\n",
      "       $5,562,800.00: citizens for emil jones\n",
      "       $5,324,649.63: paul wood,\n",
      "       $5,132,563.83: seiu healthcare il in\n",
      "ran in 3723.114373922348 seconds\n"
     ]
    }
   ],
   "source": [
    "    # Print out the number of duplicates found\n",
    "    print('# duplicate sets')\n",
    "\n",
    "    # ## Payoff\n",
    "\n",
    "    # With all this done, we can now begin to ask interesting questions\n",
    "    # of the data\n",
    "    #\n",
    "    # For example, let's see who the top 10 donors are.\n",
    "\n",
    "    locale.setlocale(locale.LC_ALL, 'en_CA.UTF-8')  # for pretty printing numbers\n",
    "    \n",
    "    athenautils.athena_start_query(\"DROP TABLE IF EXISTS as_e_map\", database=config.DATABASE)\n",
    "    \n",
    "    q = \"\"\"\n",
    "        CREATE TABLE as_e_map as \n",
    "        SELECT COALESCE(canon_id, as_entity_map.donor_id) AS canon_id, as_entity_map.donor_id \n",
    "        FROM as_entity_map \n",
    "        RIGHT JOIN as_donors USING(donor_id)        \n",
    "        \"\"\"    \n",
    "    athenautils.athena_start_query(q, database=config.DATABASE)\n",
    "    \n",
    "    q = \"\"\"\n",
    "        SELECT array_join(filter(array[as_donors.first_name, as_donors.last_name], x-> x IS NOT NULL), ' ') AS name,   \n",
    "            donation_totals.totals AS totals \n",
    "        FROM as_donors INNER JOIN \n",
    "            (SELECT canon_id, SUM(cast (amount as double)) AS totals \n",
    "            FROM as_contributions INNER JOIN as_e_map \n",
    "            USING (donor_id) \n",
    "            GROUP BY (canon_id) \n",
    "            ORDER BY totals \n",
    "            DESC LIMIT 10) \n",
    "            AS donation_totals \n",
    "        ON as_donors.donor_id = donation_totals.canon_id\n",
    "        ORDER BY totals DESC\n",
    "    \"\"\"\n",
    "    cur = athenautils.cursor_execute(q, database=config.DATABASE)\n",
    "\n",
    "    print(\"Top Donors (deduped)\")\n",
    "    for row in cur:\n",
    "        row['totals'] = locale.currency(row['totals'], grouping=True)\n",
    "        print('%(totals)20s: %(name)s' % row)\n",
    "\n",
    "    # Compare this to what we would have gotten if we hadn't done any\n",
    "    # deduplication\n",
    "    q = \"\"\"\n",
    "        with donorscontributions as(\n",
    "\n",
    "            SELECT as_donors.donor_id, \n",
    "                array_join(filter(array[as_donors.first_name, as_donors.last_name], x-> x IS NOT NULL), ' ') AS name,\n",
    "                cast(as_contributions.amount as double) as amount\n",
    "            FROM as_donors INNER JOIN as_contributions \n",
    "                USING (donor_id) \n",
    "            )\n",
    "        SELECT name, sum(amount) AS totals  \n",
    "        FROM donorscontributions\n",
    "        GROUP BY donor_id, name\n",
    "        ORDER BY totals DESC \n",
    "        LIMIT 10\n",
    "    \"\"\"\n",
    "    cur = athenautils.cursor_execute(q, database=config.DATABASE)\n",
    "\n",
    "    print(\"Top Donors (raw)\")\n",
    "    for row in cur:\n",
    "        row['totals'] = locale.currency(row['totals'], grouping=True)\n",
    "        print('%(totals)20s: %(name)s' % row)\n",
    "\n",
    "    # Close our database connection\n",
    "#     read_con.close()\n",
    "#     write_con.close()\n",
    "\n",
    "    print('ran in', time.time() - start_time, 'seconds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !jupyter nbconvert --to script athena_example.ipynb --output-dir=../athena_example/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
