{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install dedupe  pyathena"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "athena_garbage = 's3://com.ria.scratch/athena_garbage/'\n",
    "bucket='com.ria.scratch'\n",
    "region='eu-west-1'\n",
    "workgroup = 'RIA'\n",
    "root_key='as-dedupe/'\n",
    "schema_name='ria_data_science_s3'\n",
    "import sys\n",
    "sys.path.insert(0, '../../dedupe/')\n",
    "import dedupe\n",
    "from io import StringIO\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load ../mysql_example/mysql_example.py\n",
    "#!/usr/bin/python\n",
    "\n",
    "\"\"\"\n",
    "This is an example of working with very large data. There are about\n",
    "700,000 unduplicated donors in this database of Illinois political\n",
    "campaign contributions.\n",
    "\n",
    "With such a large set of input data, we cannot store all the comparisons\n",
    "we need to make in memory. Instead, we will read the pairs on demand\n",
    "from the MySQL database.\n",
    "\n",
    "__Note:__ You will need to run `python mysql_init_db.py`\n",
    "before running this script. See the annotates source for\n",
    "[mysql_init_db.py](mysql_init_db.html)\n",
    "\n",
    "For smaller datasets (<10,000), see our\n",
    "[csv_example](csv_example.html)\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import itertools\n",
    "import time\n",
    "import logging\n",
    "import optparse\n",
    "import locale\n",
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "# import MySQLdb\n",
    "# import MySQLdb.cursors\n",
    "\n",
    "import dedupe\n",
    "import dedupe.backport\n",
    "import boto3\n",
    "from pyathena import connect\n",
    "from pyathena.pandas_cursor import PandasCursor\n",
    "\n",
    "def dict_cursor_execute(cur, query):\n",
    "    df = cur.execute(query).as_pandas()\n",
    "    return df.where(pd.notnull(df), None).astype(str)\n",
    "\n",
    "\n",
    "def record_pairs(result_set):\n",
    "    for i, row in enumerate(result_set):\n",
    "        a_record_id, a_record, b_record_id, b_record = row\n",
    "        record_a = (a_record_id, json.loads(a_record))\n",
    "        record_b = (b_record_id, json.loads(b_record))\n",
    "\n",
    "        yield record_a, record_b\n",
    "\n",
    "        if i % 10000 == 0:\n",
    "            print(i)\n",
    "\n",
    "\n",
    "def cluster_ids(clustered_dupes):\n",
    "\n",
    "    for cluster, scores in clustered_dupes:\n",
    "        cluster_id = cluster[0]\n",
    "        for donor_id, score in zip(cluster, scores):\n",
    "            yield donor_id, cluster_id, score\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "\n",
    "    # ## Logging\n",
    "\n",
    "    # Dedupe uses Python logging to show or suppress verbose output. Added\n",
    "    # for convenience.  To enable verbose output, run `python\n",
    "    # examples/mysql_example/mysql_example.py -v`\n",
    "    \n",
    "#     optp = optparse.OptionParser()\n",
    "#     optp.add_option('-v', '--verbose', dest='verbose', action='count',\n",
    "#                     help='Increase verbosity (specify multiple times for more)'\n",
    "#                     )\n",
    "#     (opts, args) = optp.parse_args()\n",
    "#     log_level = logging.WARNING\n",
    "#     if opts.verbose:\n",
    "#         if opts.verbose == 1:\n",
    "#             log_level = logging.INFO\n",
    "#         elif opts.verbose >= 2:\n",
    "#             log_level = logging.DEBUG\n",
    "\n",
    "## Armin\n",
    "    log_level = logging.WARNING\n",
    "#######\n",
    "\n",
    "    logging.getLogger().setLevel(log_level)\n",
    "\n",
    "    \n",
    "\n",
    "#     # ## Setup\n",
    "#     MYSQL_CNF = os.path.abspath('.') + '/mysql.cnf'\n",
    "\n",
    "    settings_file = 'mysql_example_settings'\n",
    "    training_file = 'mysql_example_training.json'\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    # You'll need to copy `examples/mysql_example/mysql.cnf_LOCAL` to\n",
    "    # `examples/mysql_example/mysql.cnf` and fill in your mysql database\n",
    "    # information in `examples/mysql_example/mysql.cnf`\n",
    "\n",
    "    # We use Server Side cursors (SSDictCursor and SSCursor) to [avoid\n",
    "    # having to have enormous result sets in\n",
    "    # memory](http://stackoverflow.com/questions/1808150/how-to-efficiently-use-mysqldb-sscursor).\n",
    "#     read_con = MySQLdb.connect(db='contributions',\n",
    "#                                charset='utf8',\n",
    "#                                read_default_file=MYSQL_CNF,\n",
    "#                                cursorclass=MySQLdb.cursors.SSDictCursor)\n",
    "\n",
    "#     write_con = MySQLdb.connect(db='contributions',\n",
    "#                                 charset='utf8',\n",
    "#                                 read_default_file=MYSQL_CNF)\n",
    "\n",
    "    s3 = boto3.client('s3')  \n",
    "    conn = connect(s3_staging_dir=athena_garbage,\n",
    "                     region_name=region, work_group=workgroup)\n",
    "    cur = conn.cursor(PandasCursor, schema_name=schema_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !rm 'mysql_example_settings'\n",
    "# !rm 'mysql_example_training.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reading from  mysql_example_settings\n"
     ]
    }
   ],
   "source": [
    "    # We'll be using variations on this following select statement to pull\n",
    "    # in campaign donor info.\n",
    "    #\n",
    "    # We did a fair amount of preprocessing of the fields in\n",
    "    # `mysql_init_db.py`    \n",
    "    DONOR_SELECT = \"SELECT donor_id, city, name, zip, state, address \" \\\n",
    "                   \"from processed_donors\"\n",
    "\n",
    "    # ## Training\n",
    "\n",
    "    if os.path.exists(settings_file):\n",
    "        print('reading from ', settings_file)\n",
    "        with open(settings_file, 'rb') as sf:\n",
    "            deduper = dedupe.StaticDedupe(sf, num_cores=4)\n",
    "    else:\n",
    "        # Define the fields dedupe will pay attention to\n",
    "        #\n",
    "        # The address, city, and zip fields are often missing, so we'll\n",
    "        # tell dedupe that, and we'll learn a model that take that into\n",
    "        # account\n",
    "        fields = [{'field': 'name', 'type': 'String'},\n",
    "                  {'field': 'address', 'type': 'String',\n",
    "                   'has missing': True},\n",
    "                  {'field': 'city', 'type': 'ShortString', 'has missing': True},\n",
    "                  {'field': 'state', 'type': 'ShortString', 'has missing': True},\n",
    "                  {'field': 'zip', 'type': 'ShortString', 'has missing': True},\n",
    "                  ]\n",
    "\n",
    "        # Create a new deduper object and pass our data model to it.\n",
    "        deduper = dedupe.Dedupe(fields, num_cores=4)\n",
    "\n",
    "        # We will sample pairs from the entire donor table for training\n",
    "#         with read_con.cursor() as cur:\n",
    "#         cur.execute(DONOR_SELECT)\n",
    "#         temp_d = {i: row for i, row in enumerate(cur)}\n",
    "\n",
    "        # Armin: The problem is the donor_id, it's numpy's int64, should be converted to int! \n",
    "        # But for that, astype doesn't work, and a loot on temp_d is slow, so for now let's just use str\n",
    "        with conn.cursor(PandasCursor, schema_name=schema_name) as cursor:\n",
    "        #     Something like this is much faster, but let's  keep the changes minimal for now\n",
    "        #     df = cur.execute(DONOR_SELECT).as_pandas().astype(str)\n",
    "        #     temp_d = df.where(pd.notnull(df), None).to_dict('index')\n",
    "            cursor_df = dict_cursor_execute(cursor, DONOR_SELECT)\n",
    "            temp_d = cursor_df.to_dict('index')\n",
    "\n",
    "        # If we have training data saved from a previous run of dedupe,\n",
    "        # look for it an load it in.\n",
    "        #\n",
    "        # __Note:__ if you want to train from\n",
    "        # scratch, delete the training_file\n",
    "        if os.path.exists(training_file):\n",
    "            print('reading labeled examples from ', training_file)\n",
    "            with open(training_file) as tf:\n",
    "                deduper.prepare_training(temp_d, training_file=tf)\n",
    "        else:\n",
    "            deduper.prepare_training(temp_d)\n",
    "\n",
    "        del temp_d\n",
    "\n",
    "        # ## Active learning\n",
    "\n",
    "        print('starting active labeling...')\n",
    "        # Starts the training loop. Dedupe will find the next pair of records\n",
    "        # it is least certain about and ask you to label them as duplicates\n",
    "        # or not.\n",
    "\n",
    "        # use 'y', 'n' and 'u' keys to flag duplicates\n",
    "        # press 'f' when you are finished\n",
    "        dedupe.convenience.console_label(deduper)\n",
    "        # When finished, save our labeled, training pairs to disk\n",
    "        with open(training_file, 'w') as tf:\n",
    "            deduper.write_training(tf)\n",
    "\n",
    "        # Notice our the argument here\n",
    "        #\n",
    "        # `recall` is the proportion of true dupes pairs that the learned\n",
    "        # rules must cover. You may want to reduce this if your are making\n",
    "        # too many blocks and too many comparisons.\n",
    "        deduper.train(recall=0.90)\n",
    "\n",
    "        with open(settings_file, 'wb') as sf:\n",
    "            deduper.write_settings(sf)\n",
    "\n",
    "        # We can now remove some of the memory hobbing objects we used\n",
    "        # for training\n",
    "        deduper.cleanup_training()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "blocking...\n",
      "creating blocking_map database\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<pyathena.pandas_cursor.PandasCursor at 0x7fd9ce27a400>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "    # ## Blocking\n",
    "\n",
    "    print('blocking...')\n",
    "\n",
    "    # To run blocking on such a large set of data, we create a separate table\n",
    "    # that contains blocking keys and record ids\n",
    "    print('creating blocking_map database')\n",
    "#     with write_con.cursor() as cur:\n",
    "#         cur.execute(\"DROP TABLE IF EXISTS blocking_map\")\n",
    "#         cur.execute(\"CREATE TABLE blocking_map \"\n",
    "#                     \"(block_key VARCHAR(200), donor_id INTEGER) \"\n",
    "#                     \"CHARACTER SET utf8 COLLATE utf8_unicode_ci\")\n",
    "\n",
    "#     write_con.commit()\n",
    "    cur.execute(\"DROP TABLE IF EXISTS blocking_map\")\n",
    "\n",
    "    q='''\n",
    "    CREATE EXTERNAL TABLE blocking_map     \n",
    "        (block_key VARCHAR(200), donor_id INTEGER)\n",
    "    ROW FORMAT DELIMITED\n",
    "      FIELDS TERMINATED BY '\\t'\n",
    "      LINES TERMINATED BY '\\n'  \n",
    "    LOCATION\n",
    "        's3://{}/{}' \n",
    "    TBLPROPERTIES (\n",
    "        'classification'='csv', \n",
    "        --'skip.header.line.count'='1',  \n",
    "        'serialization.null.format'='')\n",
    "    '''.format(bucket, root_key+'blocking_map') \n",
    "    cur.execute(q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "creating inverted index\n"
     ]
    }
   ],
   "source": [
    "    # If dedupe learned a Index Predicate, we have to take a pass\n",
    "    # through the data and create indices.\n",
    "    print('creating inverted index')\n",
    "\n",
    "    for field in deduper.fingerprinter.index_fields:\n",
    "        q = '''\n",
    "        SELECT DISTINCT {field} FROM processed_donors \n",
    "        WHERE {field} IS NOT NULL\n",
    "        '''.format(field=field)\n",
    "        cur_df = dict_cursor_execute(cur, q)\n",
    "        # Do I need to cast it as a list?\n",
    "        field_data = cur_df[field]\n",
    "        deduper.fingerprinter.index(field_data, field)\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "writing blocking map\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'ResponseMetadata': {'RequestId': '5F215F152B811909',\n",
       "  'HostId': 'B9k8koPR2pp/7lp5WxlEM2etPGjhR3aUdlJq253YoSf1Rt6N8Jo1XAWrfe7EiplzFf++YlcW238=',\n",
       "  'HTTPStatusCode': 200,\n",
       "  'HTTPHeaders': {'x-amz-id-2': 'B9k8koPR2pp/7lp5WxlEM2etPGjhR3aUdlJq253YoSf1Rt6N8Jo1XAWrfe7EiplzFf++YlcW238=',\n",
       "   'x-amz-request-id': '5F215F152B811909',\n",
       "   'date': 'Tue, 30 Jun 2020 15:27:16 GMT',\n",
       "   'x-amz-server-side-encryption': 'AES256',\n",
       "   'etag': '\"d41d8cd98f00b204e9800998ecf8427e\"',\n",
       "   'content-length': '0',\n",
       "   'server': 'AmazonS3'},\n",
       "  'RetryAttempts': 0},\n",
       " 'ETag': '\"d41d8cd98f00b204e9800998ecf8427e\"',\n",
       " 'ServerSideEncryption': 'AES256'}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "    # Now we are ready to write our blocking map table by creating a\n",
    "    # generator that yields unique `(block_key, donor_id)` tuples.\n",
    "    print('writing blocking map')\n",
    "    \n",
    "\n",
    "    read_cur_dict = dict_cursor_execute(cur, DONOR_SELECT).to_dict('records')\n",
    "    full_data = ((row['donor_id'], row) for row in read_cur_dict)\n",
    "    b_data = deduper.fingerprinter(full_data)\n",
    "    buffer = pd.DataFrame.from_records(b_data).to_csv(index=False, header=False, sep='\\t')\n",
    "#         csv_out.writerows(b_data)        \n",
    "\n",
    "#         \"\\n\".join(b_data)\n",
    "#         with write_con.cursor() as write_cur:\n",
    "\n",
    "#             write_cur.executemany(\"INSERT INTO blocking_map VALUES (%s, %s)\",\n",
    "#                                   b_data)\n",
    "    s3.put_object(Bucket=bucket, Key=root_key+'blocking_map/blocking.csv', Body=buffer)    \n",
    "#     write_con.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "    # indexing blocking_map\n",
    "#     print('creating index')\n",
    "#     with write_con.cursor() as cur:\n",
    "#         cur.execute(\"CREATE UNIQUE INDEX bm_idx ON blocking_map (block_key, donor_id)\")\n",
    "\n",
    "#     write_con.commit()\n",
    "#     read_con.commit()\n",
    "\n",
    "    # select unique pairs to compare\n",
    "    q='''\n",
    "    SELECT a.donor_id,\n",
    "        json_format(CAST (MAP(ARRAY['city', 'name', 'zip', 'state', 'address'],\n",
    "                              ARRAY[ a.city, a.name, a.zip, a.state, a.address])\n",
    "                    AS JSON)),\n",
    "        b.donor_id,\n",
    "        json_format(CAST (MAP(ARRAY['city', 'name', 'zip', 'state', 'address'], \n",
    "                  ARRAY[ b.city, b.name, b.zip, b.state, b.address])\n",
    "              AS JSON))\n",
    "    FROM (SELECT DISTINCT l.donor_id as east, r.donor_id as west\n",
    "         from blocking_map as l\n",
    "         INNER JOIN blocking_map as r\n",
    "         using (block_key)\n",
    "         where l.donor_id < r.donor_id) ids\n",
    "    INNER JOIN processed_donors a on ids.east=a.donor_id\n",
    "    INNER JOIN processed_donors b on ids.west=b.donor_id\n",
    "    '''\n",
    "    read_cur_dict=dict_cursor_execute(cur, q).itertuples(index=False, name=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "StopIteration",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mStopIteration\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-bb1ab6348ed6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mread_cur_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mStopIteration\u001b[0m: "
     ]
    }
   ],
   "source": [
    "next(read_cur_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "clustering...\n"
     ]
    },
    {
     "ename": "BlockingError",
     "evalue": "No records have been blocked together. Is the data you are trying to match like the data you trained on?",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mBlockingError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-186191d0dae6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'clustering...'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m clustered_dupes = deduper.cluster(deduper.score(record_pairs(read_cur_dict)),\n\u001b[0m\u001b[1;32m      5\u001b[0m                                       threshold=0.5)\n",
      "\u001b[0;32m~/anaconda3/envs/python3/lib/python3.6/site-packages/dedupe/api.py\u001b[0m in \u001b[0;36mscore\u001b[0;34m(self, pairs)\u001b[0m\n\u001b[1;32m    104\u001b[0m                                            \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata_model\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m                                            \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclassifier\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 106\u001b[0;31m                                            self.num_cores)\n\u001b[0m\u001b[1;32m    107\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m             raise RuntimeError('''\n",
      "\u001b[0;32m~/anaconda3/envs/python3/lib/python3.6/site-packages/dedupe/core.py\u001b[0m in \u001b[0;36mscoreDuplicates\u001b[0;34m(record_pairs, data_model, classifier, num_cores)\u001b[0m\n\u001b[1;32m    218\u001b[0m     \u001b[0mfirst\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecord_pairs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpeek\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrecord_pairs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    219\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfirst\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 220\u001b[0;31m         raise BlockingError(\"No records have been blocked together. \"\n\u001b[0m\u001b[1;32m    221\u001b[0m                             \u001b[0;34m\"Is the data you are trying to match like \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    222\u001b[0m                             \"the data you trained on?\")\n",
      "\u001b[0;31mBlockingError\u001b[0m: No records have been blocked together. Is the data you are trying to match like the data you trained on?"
     ]
    }
   ],
   "source": [
    "    # ## Clustering\n",
    "\n",
    "    print('clustering...')\n",
    "    clustered_dupes = deduper.cluster(deduper.score(record_pairs(read_cur_dict)),\n",
    "                                          threshold=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    cur.execute(\"DROP TABLE IF EXISTS entity_map\")\n",
    "\n",
    "    print('creating entity_map database')\n",
    "    q='''\n",
    "    CREATE EXTERNAL TABLE entity_map     \n",
    "        (donor_id INTEGER, canon_id INTEGER, \n",
    "         cluster_score FLOAT)\n",
    "    ROW FORMAT DELIMITED\n",
    "      FIELDS TERMINATED BY '\\t'\n",
    "      LINES TERMINATED BY '\\n'  \n",
    "    LOCATION\n",
    "        's3://{}/{}' \n",
    "    TBLPROPERTIES (\n",
    "        'classification'='csv', \n",
    "        --'skip.header.line.count'='1',  \n",
    "        'serialization.null.format'='')\n",
    "    '''.format(bucket, root_key+'entity_map') \n",
    "    cur.execute(q) \n",
    "\n",
    "    buffer = pd.DataFrame.from_records(cluster_ids(clustered_dupes)).to_csv(index=False, header=False, sep='\\t')\n",
    "    s3.put_object(Bucket=bucket, Key=root_key+'entity_map/entity_map.csv', Body=buffer)    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    # Print out the number of duplicates found\n",
    "    print('# duplicate sets')\n",
    "\n",
    "    # ## Payoff\n",
    "\n",
    "    # With all this done, we can now begin to ask interesting questions\n",
    "    # of the data\n",
    "    #\n",
    "    # For example, let's see who the top 10 donors are.\n",
    "\n",
    "    locale.setlocale(locale.LC_ALL, '')  # for pretty printing numbers\n",
    "    \n",
    "    cur.execute(\"DROP TABLE IF EXISTS e_map\")\n",
    "\n",
    "    q = '''\n",
    "    CREATE TABLE e_map as \n",
    "        SELECT COALESCE(canon_id, entity_map.donor_id) AS canon_id, entity_map.donor_id \n",
    "        FROM entity_map \n",
    "            RIGHT JOIN donors USING(donor_id)\n",
    "    '''\n",
    "    \n",
    "    cur.execute(q)\n",
    "    q ='''\n",
    "    SELECT array_join(filter(array[donors.first_name, donors.last_name], x-> x IS NOT NULL), ' ') AS name,   \n",
    "        donation_totals.totals AS totals \n",
    "    FROM donors INNER JOIN \n",
    "        (SELECT canon_id, SUM(cast (amount as double)) AS totals \n",
    "        FROM contributions INNER JOIN e_map \n",
    "        USING (donor_id) \n",
    "        GROUP BY (canon_id) \n",
    "        ORDER BY totals \n",
    "        DESC LIMIT 10) \n",
    "        AS donation_totals \n",
    "    ON donors.donor_id = donation_totals.canon_id\n",
    "    '''\n",
    "    cur_dict = dict_cursor_execute(cur, q).to_dict('records')\n",
    "\n",
    "    print(\"Top Donors (deduped)\")\n",
    "    for row in cur_dict:\n",
    "        row['totals'] = locale.currency(row['totals'], grouping=True)\n",
    "        print('%(totals)20s: %(name)s' % row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    # Compare this to what we would have gotten if we hadn't done any\n",
    "    # deduplication\n",
    "\n",
    "    q = '''\n",
    "    SELECT array_join(filter(array[donors.first_name, donors.last_name], x-> x IS NOT NULL), ' ') AS name,\n",
    "        SUM(cast(contributions.amount as double)) AS totals \n",
    "    FROM donors INNER JOIN contributions \n",
    "        USING (donor_id) \n",
    "    GROUP BY donor_id), name\n",
    "    ORDER BY totals DESC \n",
    "    LIMIT 10\")\n",
    "    '''\n",
    "\n",
    "    cur_dict = dict_cursor_execute(cur, q).to_dict('records')\n",
    "\n",
    "    print(\"Top Donors (raw)\")\n",
    "    for row in cur:\n",
    "        row['totals'] = locale.currency(row['totals'], grouping=True)\n",
    "        print('%(totals)20s: %(name)s' % row)\n",
    "\n",
    "    # Close our database connection\n",
    "#     read_con.close()\n",
    "#     write_con.close()\n",
    "\n",
    "    print('ran in', time.time() - start_time, 'seconds')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
