{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting dedupe\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/5e/09/179feb316147279c76ea7e6dc5a5f9e00a6feadaeda131d535247e580619/dedupe-2.0.3-cp36-cp36m-manylinux1_x86_64.whl (89kB)\n",
      "\u001b[K    100% |████████████████████████████████| 92kB 239kB/s ta 0:00:011\n",
      "\u001b[?25hCollecting pyathena\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/40/85/f37c049922f5d47e9126d7817ef7b8fb7abb2e6a9ea0dd06adcbffc0e8bc/PyAthena-1.10.8-py2.py3-none-any.whl (53kB)\n",
      "\u001b[K    100% |████████████████████████████████| 61kB 1.9MB/s ta 0:00:011\n",
      "\u001b[?25hCollecting haversine>=0.4.1 (from dedupe)\n",
      "  Downloading https://files.pythonhosted.org/packages/72/8e/6df8b563dd6b2961a36cd740b34c00b89142f1b97d92092c133379b2973f/haversine-2.2.0-py2.py3-none-any.whl\n",
      "Collecting simplecosine>=1.2 (from dedupe)\n",
      "  Downloading https://files.pythonhosted.org/packages/2d/22/6ea3a5ab8aea06d6563eb927e706f7342a00d1849c9be6143a2a7d84ddbd/simplecosine-1.2-py2.py3-none-any.whl\n",
      "Collecting rlr>=2.4.3 (from dedupe)\n",
      "  Downloading https://files.pythonhosted.org/packages/fa/02/3b1a9727a622ff4320919645ce35ceb887d90784d0bab41484756c33b7ea/rlr-2.4.5-py2.py3-none-any.whl\n",
      "Collecting categorical-distance>=1.9 (from dedupe)\n",
      "  Downloading https://files.pythonhosted.org/packages/1d/b7/4f97771f52c63916f4e4d349a644c2387961592e76070e7310463b2d70a5/categorical_distance-1.9-py3-none-any.whl\n",
      "Requirement already satisfied: numpy>=1.13 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from dedupe) (1.14.3)\n",
      "Collecting fastcluster (from dedupe)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/1e/9d/3d7525a4722ee4a11ad969762d1de53b6dac326b5ac1366221e06958e1d7/fastcluster-1.1.26-cp36-cp36m-manylinux1_x86_64.whl (154kB)\n",
      "\u001b[K    100% |████████████████████████████████| 163kB 707kB/s ta 0:00:01\n",
      "\u001b[?25hCollecting highered>=0.2.0 (from dedupe)\n",
      "  Downloading https://files.pythonhosted.org/packages/81/00/cbd902cfd14ad1992fcdaa11a615d47b36b6136dc690e19b0afa58c7365d/highered-0.2.1-py2.py3-none-any.whl\n",
      "Collecting dedupe-hcluster (from dedupe)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/b2/1f/c6f6075c2e988b3a1759fabaf91d2f8f2de59c6e607a3fd9a2e06112a0de/dedupe_hcluster-0.3.8-cp36-cp36m-manylinux1_x86_64.whl (531kB)\n",
      "\u001b[K    100% |████████████████████████████████| 532kB 5.2MB/s ta 0:00:01\n",
      "\u001b[?25hCollecting BTrees>=4.1.4 (from dedupe)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/48/b3/9ce3b32817db98e8bf20d6873e18ee3ee7feded135434d800b72bf8dfb9f/BTrees-4.7.2-cp36-cp36m-manylinux1_x86_64.whl (3.0MB)\n",
      "\u001b[K    100% |████████████████████████████████| 3.0MB 8.2MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting Levenshtein-search (from dedupe)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/93/89/dc320196d10447540c95f58eab5dd316a2166310356c1d88b84724f4e793/Levenshtein_search-1.4.5-cp36-cp36m-manylinux1_x86_64.whl (59kB)\n",
      "\u001b[K    100% |████████████████████████████████| 61kB 21.2MB/s ta 0:00:01\n",
      "\u001b[?25hCollecting zope.index (from dedupe)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ab/0f/f93bddfac1189bb6b973142da3ef2caa6817a59b07ca448095a30b644737/zope.index-5.0.0-cp36-cp36m-manylinux1_x86_64.whl (101kB)\n",
      "\u001b[K    100% |████████████████████████████████| 102kB 17.6MB/s a 0:00:01\n",
      "\u001b[?25hCollecting typing-extensions (from dedupe)\n",
      "  Downloading https://files.pythonhosted.org/packages/0c/0e/3f026d0645d699e7320b59952146d56ad7c374e9cd72cd16e7c74e657a0f/typing_extensions-3.7.4.2-py3-none-any.whl\n",
      "Collecting affinegap>=1.3 (from dedupe)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/b2/6a/91f5defe8178104449bc897208c9780b159575d16a959a5074f0bf39a6f0/affinegap-1.11-cp36-cp36m-manylinux1_x86_64.whl (45kB)\n",
      "\u001b[K    100% |████████████████████████████████| 51kB 12.0MB/s ta 0:00:01\n",
      "\u001b[?25hCollecting doublemetaphone (from dedupe)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/c0/27/8df369334aac64755ca899b9a7cc4d2d60e800cca148322ef19309cdae0f/DoubleMetaphone-0.1-cp36-cp36m-manylinux1_x86_64.whl (78kB)\n",
      "\u001b[K    100% |████████████████████████████████| 81kB 3.4MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting dedupe-variable-datetime (from dedupe)\n",
      "  Downloading https://files.pythonhosted.org/packages/65/8f/d21f6acadcdfd681ee038153883b5673b8b76f790e465d791780e6b7bf60/dedupe_variable_datetime-0.1.5-py3-none-any.whl\n",
      "Collecting tenacity>=4.1.0 (from pyathena)\n",
      "  Downloading https://files.pythonhosted.org/packages/b5/05/ff089032442058bd3386f9cd991cd88ccac81dca1494d78751621ee35e62/tenacity-6.2.0-py2.py3-none-any.whl\n",
      "Requirement already satisfied: botocore>=1.5.52 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from pyathena) (1.15.39)\n",
      "Collecting future (from pyathena)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/45/0b/38b06fd9b92dc2b68d58b75f900e97884c45bedd2ff83203d933cf5851c9/future-0.18.2.tar.gz (829kB)\n",
      "\u001b[K    100% |████████████████████████████████| 829kB 14.2MB/s ta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: boto3>=1.4.4 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from pyathena) (1.12.39)\n",
      "Collecting pylbfgs (from rlr>=2.4.3->dedupe)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/b8/5b/b8e1ef62e5e5b034ce5ae919b64158ec8da4f64c995444aec7fd96e8ec42/PyLBFGS-0.2.0.13-cp36-cp36m-manylinux1_x86_64.whl (205kB)\n",
      "\u001b[K    100% |████████████████████████████████| 215kB 16.4MB/s ta 0:00:01\n",
      "\u001b[?25hCollecting pyhacrf-datamade>=0.2.0 (from highered>=0.2.0->dedupe)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/84/f5/971e17a8b6686d5fc3d562e29e9c902743eb5f0f4436880b86cb11c0149c/pyhacrf_datamade-0.2.5-cp36-cp36m-manylinux1_x86_64.whl (788kB)\n",
      "\u001b[K    100% |████████████████████████████████| 798kB 14.5MB/s ta 0:00:01\n",
      "\u001b[?25hCollecting zope.interface (from BTrees>=4.1.4->dedupe)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/fc/7e/8e1efcfa22b722a0d6e992172ab15a871988c290cb722fe8da6d11f1aeb2/zope.interface-5.1.0-cp36-cp36m-manylinux1_x86_64.whl (234kB)\n",
      "\u001b[K    100% |████████████████████████████████| 235kB 16.6MB/s ta 0:00:01\n",
      "\u001b[?25hCollecting persistent>=4.1.0 (from BTrees>=4.1.4->dedupe)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/2e/4e/9bde9a2f63273f2e63a94a8198781aac559cc6efd2f560d69afcb0d9d8b5/persistent-4.6.4-cp36-cp36m-manylinux1_x86_64.whl (246kB)\n",
      "\u001b[K    100% |████████████████████████████████| 256kB 17.5MB/s ta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: six in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from zope.index->dedupe) (1.11.0)\n",
      "Requirement already satisfied: setuptools in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from zope.index->dedupe) (39.1.0)\n",
      "Collecting datetime-distance (from dedupe-variable-datetime->dedupe)\n",
      "  Downloading https://files.pythonhosted.org/packages/6b/98/a5eff9256ff27e3bb8030466dabd772002e5014b9237cbeb18c542050ff5/datetime_distance-0.1.3-py3-none-any.whl\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from botocore>=1.5.52->pyathena) (2.7.3)\n",
      "Requirement already satisfied: docutils<0.16,>=0.10 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from botocore>=1.5.52->pyathena) (0.14)\n",
      "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from botocore>=1.5.52->pyathena) (0.9.4)\n",
      "Requirement already satisfied: urllib3<1.26,>=1.20; python_version != \"3.4\" in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from botocore>=1.5.52->pyathena) (1.23)\n",
      "Requirement already satisfied: s3transfer<0.4.0,>=0.3.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from boto3>=1.4.4->pyathena) (0.3.3)\n",
      "Requirement already satisfied: cffi; platform_python_implementation == \"CPython\" in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from persistent>=4.1.0->BTrees>=4.1.4->dedupe) (1.11.5)\n",
      "Requirement already satisfied: pycparser in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from cffi; platform_python_implementation == \"CPython\"->persistent>=4.1.0->BTrees>=4.1.4->dedupe) (2.18)\n",
      "Building wheels for collected packages: future\n",
      "  Running setup.py bdist_wheel for future ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /home/ec2-user/.cache/pip/wheels/8b/99/a0/81daf51dcd359a9377b110a8a886b3895921802d2fc1b2397e\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully built future\n",
      "Installing collected packages: haversine, simplecosine, future, pylbfgs, rlr, categorical-distance, fastcluster, pyhacrf-datamade, highered, dedupe-hcluster, zope.interface, persistent, BTrees, Levenshtein-search, zope.index, typing-extensions, affinegap, doublemetaphone, datetime-distance, dedupe-variable-datetime, dedupe, tenacity, pyathena\n",
      "Successfully installed BTrees-4.7.2 Levenshtein-search-1.4.5 affinegap-1.11 categorical-distance-1.9 datetime-distance-0.1.3 dedupe-2.0.3 dedupe-hcluster-0.3.8 dedupe-variable-datetime-0.1.5 doublemetaphone-0.1 fastcluster-1.1.26 future-0.18.2 haversine-2.2.0 highered-0.2.1 persistent-4.6.4 pyathena-1.10.8 pyhacrf-datamade-0.2.5 pylbfgs-0.2.0.13 rlr-2.4.5 simplecosine-1.2 tenacity-6.2.0 typing-extensions-3.7.4.2 zope.index-5.0.0 zope.interface-5.1.0\n",
      "\u001b[33mYou are using pip version 10.0.1, however version 20.2b1 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install dedupe  pyathena"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, '../athena_example/')\n",
    "import config\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'logging' has no attribute 'logging'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-878b2de91830>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     85\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m \u001b[0;31m## Armin\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 87\u001b[0;31m     \u001b[0mlog_level\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlogging\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogging\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDEBUG\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     88\u001b[0m \u001b[0;31m#######\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'logging' has no attribute 'logging'"
     ]
    }
   ],
   "source": [
    "# %load ../mysql_example/mysql_example.py\n",
    "#!/usr/bin/python\n",
    "\n",
    "\"\"\"\n",
    "This is an example of working with very large data. There are about\n",
    "700,000 unduplicated donors in this database of Illinois political\n",
    "campaign contributions.\n",
    "\n",
    "With such a large set of input data, we cannot store all the comparisons\n",
    "we need to make in memory. Instead, we will read the pairs on demand\n",
    "from the MySQL database.\n",
    "\n",
    "__Note:__ You will need to run `python mysql_init_db.py`\n",
    "before running this script. See the annotates source for\n",
    "[mysql_init_db.py](mysql_init_db.html)\n",
    "\n",
    "For smaller datasets (<10,000), see our\n",
    "[csv_example](csv_example.html)\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import itertools\n",
    "import time\n",
    "import logging\n",
    "import optparse\n",
    "import locale\n",
    "import json\n",
    "from io import StringIO\n",
    "import csv\n",
    "import pandas as pd\n",
    "\n",
    "# import MySQLdb\n",
    "# import MySQLdb.cursors\n",
    "\n",
    "import boto3\n",
    "from pyathena import connect\n",
    "from pyathena.pandas_cursor import PandasCursor\n",
    "import dedupe\n",
    "import dedupe.backport\n",
    "\n",
    "def dict_cursor_execute(cur, query):\n",
    "    df = cur.execute(query).as_pandas()\n",
    "    return df.where(pd.notnull(df), None).astype(str)\n",
    "\n",
    "\n",
    "def record_pairs(result_set):\n",
    "    for i, row in enumerate(result_set):\n",
    "        a_record_id, a_record, b_record_id, b_record = row\n",
    "        record_a = (a_record_id, json.loads(a_record))\n",
    "        record_b = (b_record_id, json.loads(b_record))\n",
    "\n",
    "        yield record_a, record_b\n",
    "\n",
    "        if i % 10000 == 0:\n",
    "            print(i)\n",
    "\n",
    "\n",
    "def cluster_ids(clustered_dupes):\n",
    "\n",
    "    for cluster, scores in clustered_dupes:\n",
    "        cluster_id = cluster[0]\n",
    "        for donor_id, score in zip(cluster, scores):\n",
    "            yield donor_id, cluster_id, score\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "\n",
    "    # ## Logging\n",
    "\n",
    "    # Dedupe uses Python logging to show or suppress verbose output. Added\n",
    "    # for convenience.  To enable verbose output, run `python\n",
    "    # examples/mysql_example/mysql_example.py -v`\n",
    "    \n",
    "#     optp = optparse.OptionParser()\n",
    "#     optp.add_option('-v', '--verbose', dest='verbose', action='count',\n",
    "#                     help='Increase verbosity (specify multiple times for more)'\n",
    "#                     )\n",
    "#     (opts, args) = optp.parse_args()\n",
    "#     log_level = logging.WARNING\n",
    "#     if opts.verbose:\n",
    "#         if opts.verbose == 1:\n",
    "#             log_level = logging.INFO\n",
    "#         elif opts.verbose >= 2:\n",
    "#             log_level = logging.DEBUG\n",
    "\n",
    "## Armin\n",
    "    log_level = logging.DEBUG\n",
    "#######\n",
    "\n",
    "    logging.getLogger().setLevel(log_level)\n",
    "\n",
    "    \n",
    "\n",
    "#     # ## Setup\n",
    "#     MYSQL_CNF = os.path.abspath('.') + '/mysql.cnf'\n",
    "\n",
    "    settings_file = 'mysql_example_settings'\n",
    "    training_file = 'mysql_example_training.json'\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    # You'll need to copy `examples/mysql_example/mysql.cnf_LOCAL` to\n",
    "    # `examples/mysql_example/mysql.cnf` and fill in your mysql database\n",
    "    # information in `examples/mysql_example/mysql.cnf`\n",
    "\n",
    "    # We use Server Side cursors (SSDictCursor and SSCursor) to [avoid\n",
    "    # having to have enormous result sets in\n",
    "    # memory](http://stackoverflow.com/questions/1808150/how-to-efficiently-use-mysqldb-sscursor).\n",
    "#     read_con = MySQLdb.connect(db='contributions',\n",
    "#                                charset='utf8',\n",
    "#                                read_default_file=MYSQL_CNF,\n",
    "#                                cursorclass=MySQLdb.cursors.SSDictCursor)\n",
    "\n",
    "#     write_con = MySQLdb.connect(db='contributions',\n",
    "#                                 charset='utf8',\n",
    "#                                 read_default_file=MYSQL_CNF)\n",
    "\n",
    "    s3 = boto3.client('s3')  \n",
    "    conn = connect(aws_access_key_id=config.ACCESS_KEY_ID,\n",
    "                   aws_secret_access_key=config.SECRET_ACCESS_KEY,\n",
    "                   s3_staging_dir=config.ATHENA_GARBAGE_PATH,\n",
    "                   region_name=config.REGION, \n",
    "                   work_group=config.WORKGROUP)    \n",
    "    cur = conn.cursor(PandasCursor, schema_name=config.SCHEMA_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !rm 'mysql_example_settings'\n",
    "# !rm 'mysql_example_training.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    # We'll be using variations on this following select statement to pull\n",
    "    # in campaign donor info.\n",
    "    #\n",
    "    # We did a fair amount of preprocessing of the fields in\n",
    "    # `mysql_init_db.py`    \n",
    "    DONOR_SELECT = \"SELECT donor_id, city, name, zip, state, address \" \\\n",
    "                   \"from processed_donors\"\n",
    "\n",
    "    # ## Training\n",
    "\n",
    "    if os.path.exists(settings_file):\n",
    "        print('reading from ', settings_file)\n",
    "        with open(settings_file, 'rb') as sf:\n",
    "            deduper = dedupe.StaticDedupe(sf, num_cores=4)\n",
    "    else:\n",
    "        # Define the fields dedupe will pay attention to\n",
    "        #\n",
    "        # The address, city, and zip fields are often missing, so we'll\n",
    "        # tell dedupe that, and we'll learn a model that take that into\n",
    "        # account\n",
    "        fields = [{'field': 'name', 'type': 'String'},\n",
    "                  {'field': 'address', 'type': 'String',\n",
    "                   'has missing': True},\n",
    "                  {'field': 'city', 'type': 'ShortString', 'has missing': True},\n",
    "                  {'field': 'state', 'type': 'ShortString', 'has missing': True},\n",
    "                  {'field': 'zip', 'type': 'ShortString', 'has missing': True},\n",
    "                  ]\n",
    "\n",
    "        # Create a new deduper object and pass our data model to it.\n",
    "        deduper = dedupe.Dedupe(fields, num_cores=4)\n",
    "\n",
    "        # We will sample pairs from the entire donor table for training\n",
    "#         with read_con.cursor() as cur:\n",
    "#         cur.execute(DONOR_SELECT)\n",
    "#         temp_d = {i: row for i, row in enumerate(cur)}\n",
    "\n",
    "        # Armin: The problem is the donor_id, it's numpy's int64, should be converted to int! \n",
    "        # But for that, astype doesn't work, and a loot on temp_d is slow, so for now let's just use str\n",
    "        with conn.cursor(PandasCursor, schema_name=schema_name) as cursor:\n",
    "        #     Something like this is much faster, but let's  keep the changes minimal for now\n",
    "        #     df = cur.execute(DONOR_SELECT).as_pandas().astype(str)\n",
    "        #     temp_d = df.where(pd.notnull(df), None).to_dict('index')\n",
    "            cursor_df = dict_cursor_execute(cursor, DONOR_SELECT)\n",
    "            temp_d = cursor_df.to_dict('index')\n",
    "\n",
    "        # If we have training data saved from a previous run of dedupe,\n",
    "        # look for it an load it in.\n",
    "        #\n",
    "        # __Note:__ if you want to train from\n",
    "        # scratch, delete the training_file\n",
    "        if os.path.exists(training_file):\n",
    "            print('reading labeled examples from ', training_file)\n",
    "            with open(training_file) as tf:\n",
    "                deduper.prepare_training(temp_d, training_file=tf)\n",
    "        else:\n",
    "            deduper.prepare_training(temp_d)\n",
    "\n",
    "        del temp_d\n",
    "\n",
    "        # ## Active learning\n",
    "\n",
    "        print('starting active labeling...')\n",
    "        # Starts the training loop. Dedupe will find the next pair of records\n",
    "        # it is least certain about and ask you to label them as duplicates\n",
    "        # or not.\n",
    "\n",
    "        # use 'y', 'n' and 'u' keys to flag duplicates\n",
    "        # press 'f' when you are finished\n",
    "        dedupe.convenience.console_label(deduper)\n",
    "        # When finished, save our labeled, training pairs to disk\n",
    "        with open(training_file, 'w') as tf:\n",
    "            deduper.write_training(tf)\n",
    "\n",
    "        # Notice our the argument here\n",
    "        #\n",
    "        # `recall` is the proportion of true dupes pairs that the learned\n",
    "        # rules must cover. You may want to reduce this if your are making\n",
    "        # too many blocks and too many comparisons.\n",
    "        deduper.train(recall=0.90)\n",
    "\n",
    "        with open(settings_file, 'wb') as sf:\n",
    "            deduper.write_settings(sf)\n",
    "\n",
    "        # We can now remove some of the memory hobbing objects we used\n",
    "        # for training\n",
    "        deduper.cleanup_training()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    # ## Blocking\n",
    "\n",
    "    print('blocking...')\n",
    "\n",
    "    # To run blocking on such a large set of data, we create a separate table\n",
    "    # that contains blocking keys and record ids\n",
    "    print('creating blocking_map database')\n",
    "#     with write_con.cursor() as cur:\n",
    "#         cur.execute(\"DROP TABLE IF EXISTS blocking_map\")\n",
    "#         cur.execute(\"CREATE TABLE blocking_map \"\n",
    "#                     \"(block_key VARCHAR(200), donor_id INTEGER) \"\n",
    "#                     \"CHARACTER SET utf8 COLLATE utf8_unicode_ci\")\n",
    "\n",
    "#     write_con.commit()\n",
    "    cur.execute(\"DROP TABLE IF EXISTS blocking_map\")\n",
    "\n",
    "    q='''\n",
    "    CREATE EXTERNAL TABLE blocking_map     \n",
    "        (block_key VARCHAR(200), donor_id INTEGER)\n",
    "    ROW FORMAT DELIMITED\n",
    "      FIELDS TERMINATED BY '\\t'\n",
    "      LINES TERMINATED BY '\\n'  \n",
    "    LOCATION\n",
    "        's3://{}/{}' \n",
    "    TBLPROPERTIES (\n",
    "        'classification'='csv', \n",
    "        --'skip.header.line.count'='1',  \n",
    "        'serialization.null.format'='')\n",
    "    '''.format(config.DATABASE_BUCKET, config.DATABASE_ROOT_KEY+'blocking_map') \n",
    "    cur.execute(q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    # If dedupe learned a Index Predicate, we have to take a pass\n",
    "    # through the data and create indices.\n",
    "    print('creating inverted index')\n",
    "\n",
    "    for field in deduper.fingerprinter.index_fields:\n",
    "        q = '''\n",
    "        SELECT DISTINCT {field} FROM processed_donors \n",
    "        WHERE {field} IS NOT NULL\n",
    "        '''.format(field=field)\n",
    "        cur_df = dict_cursor_execute(cur, q)\n",
    "        # Do I need to cast it as a list?\n",
    "        field_data = cur_df[field]\n",
    "        deduper.fingerprinter.index(field_data, field)\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    # Now we are ready to write our blocking map table by creating a\n",
    "    # generator that yields unique `(block_key, donor_id)` tuples.\n",
    "    print('writing blocking map')\n",
    "    \n",
    "\n",
    "    read_cur_dict = dict_cursor_execute(cur, DONOR_SELECT).to_dict('records')\n",
    "    full_data = ((row['donor_id'], row) for row in read_cur_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    b_data = deduper.fingerprinter(full_data)\n",
    "    buffer = pd.DataFrame.from_records(b_data).to_csv(index=False, header=False, sep='\\t')\n",
    "#         csv_out.writerows(b_data)        \n",
    "\n",
    "#         \"\\n\".join(b_data)\n",
    "#         with write_con.cursor() as write_cur:\n",
    "\n",
    "#             write_cur.executemany(\"INSERT INTO blocking_map VALUES (%s, %s)\",\n",
    "#                                   b_data)\n",
    "    s3.put_object(Bucket=config.DATABASE_BUCKET, Key=config.DATABASE_ROOT_KEY+'blocking_map/blocking.csv', Body=buffer)    \n",
    "#     write_con.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    # indexing blocking_map\n",
    "#     print('creating index')\n",
    "#     with write_con.cursor() as cur:\n",
    "#         cur.execute(\"CREATE UNIQUE INDEX bm_idx ON blocking_map (block_key, donor_id)\")\n",
    "\n",
    "#     write_con.commit()\n",
    "#     read_con.commit()\n",
    "\n",
    "    # select unique pairs to compare\n",
    "    q='''\n",
    "    SELECT a.donor_id,\n",
    "        json_format(CAST (MAP(ARRAY['city', 'name', 'zip', 'state', 'address'],\n",
    "                              ARRAY[ a.city, a.name, a.zip, a.state, a.address])\n",
    "                    AS JSON)),\n",
    "        b.donor_id,\n",
    "        json_format(CAST (MAP(ARRAY['city', 'name', 'zip', 'state', 'address'], \n",
    "                  ARRAY[ b.city, b.name, b.zip, b.state, b.address])\n",
    "              AS JSON))\n",
    "    FROM (SELECT DISTINCT l.donor_id as east, r.donor_id as west\n",
    "         from blocking_map as l\n",
    "         INNER JOIN blocking_map as r\n",
    "         using (block_key)\n",
    "         where l.donor_id < r.donor_id) ids\n",
    "    INNER JOIN processed_donors a on ids.east=a.donor_id\n",
    "    INNER JOIN processed_donors b on ids.west=b.donor_id\n",
    "    '''\n",
    "    read_cur_dict=dict_cursor_execute(cur, q).itertuples(index=False, name=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    # ## Clustering\n",
    "\n",
    "    print('clustering...')\n",
    "    clustered_dupes = deduper.cluster(deduper.score(record_pairs(read_cur_dict)),\n",
    "                                          threshold=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    cur.execute(\"DROP TABLE IF EXISTS entity_map\")\n",
    "\n",
    "    print('creating entity_map database')\n",
    "    q='''\n",
    "    CREATE EXTERNAL TABLE entity_map     \n",
    "        (donor_id INTEGER, canon_id INTEGER, \n",
    "         cluster_score FLOAT)\n",
    "    ROW FORMAT DELIMITED\n",
    "      FIELDS TERMINATED BY '\\t'\n",
    "      LINES TERMINATED BY '\\n'  \n",
    "    LOCATION\n",
    "        's3://{}/{}' \n",
    "    TBLPROPERTIES (\n",
    "        'classification'='csv', \n",
    "        --'skip.header.line.count'='1',  \n",
    "        'serialization.null.format'='')\n",
    "    '''.format(bucket, root_key+'entity_map') \n",
    "    cur.execute(q) \n",
    "\n",
    "    buffer = pd.DataFrame.from_records(cluster_ids(clustered_dupes)).to_csv(index=False, header=False, sep='\\t')\n",
    "    s3.put_object(Bucket=bucket, Key=root_key+'entity_map/entity_map.csv', Body=buffer)    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    # Print out the number of duplicates found\n",
    "    print('# duplicate sets')\n",
    "\n",
    "    # ## Payoff\n",
    "\n",
    "    # With all this done, we can now begin to ask interesting questions\n",
    "    # of the data\n",
    "    #\n",
    "    # For example, let's see who the top 10 donors are.\n",
    "\n",
    "    locale.setlocale(locale.LC_ALL, '')  # for pretty printing numbers\n",
    "    \n",
    "    cur.execute(\"DROP TABLE IF EXISTS e_map\")\n",
    "\n",
    "    q = '''\n",
    "    CREATE TABLE e_map as \n",
    "        SELECT COALESCE(canon_id, entity_map.donor_id) AS canon_id, entity_map.donor_id \n",
    "        FROM entity_map \n",
    "            RIGHT JOIN donors USING(donor_id)\n",
    "    '''\n",
    "    \n",
    "    cur.execute(q)\n",
    "    q ='''\n",
    "    SELECT array_join(filter(array[donors.first_name, donors.last_name], x-> x IS NOT NULL), ' ') AS name,   \n",
    "        donation_totals.totals AS totals \n",
    "    FROM donors INNER JOIN \n",
    "        (SELECT canon_id, SUM(cast (amount as double)) AS totals \n",
    "        FROM contributions INNER JOIN e_map \n",
    "        USING (donor_id) \n",
    "        GROUP BY (canon_id) \n",
    "        ORDER BY totals \n",
    "        DESC LIMIT 10) \n",
    "        AS donation_totals \n",
    "    ON donors.donor_id = donation_totals.canon_id\n",
    "    '''\n",
    "    cur_dict = dict_cursor_execute(cur, q).to_dict('records')\n",
    "\n",
    "    print(\"Top Donors (deduped)\")\n",
    "    for row in cur_dict:\n",
    "        row['totals'] = locale.currency(row['totals'], grouping=True)\n",
    "        print('%(totals)20s: %(name)s' % row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    # Compare this to what we would have gotten if we hadn't done any\n",
    "    # deduplication\n",
    "\n",
    "    q = '''\n",
    "    SELECT array_join(filter(array[donors.first_name, donors.last_name], x-> x IS NOT NULL), ' ') AS name,\n",
    "        SUM(cast(contributions.amount as double)) AS totals \n",
    "    FROM donors INNER JOIN contributions \n",
    "        USING (donor_id) \n",
    "    GROUP BY donor_id), name\n",
    "    ORDER BY totals DESC \n",
    "    LIMIT 10\")\n",
    "    '''\n",
    "\n",
    "    cur_dict = dict_cursor_execute(cur, q).to_dict('records')\n",
    "\n",
    "    print(\"Top Donors (raw)\")\n",
    "    for row in cur:\n",
    "        row['totals'] = locale.currency(row['totals'], grouping=True)\n",
    "        print('%(totals)20s: %(name)s' % row)\n",
    "\n",
    "    # Close our database connection\n",
    "#     read_con.close()\n",
    "#     write_con.close()\n",
    "\n",
    "    print('ran in', time.time() - start_time, 'seconds')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# I'm here\n",
    "Found a way to map block_key to block_numbers\n",
    "** CREATE TABLE, according to some thing online, has more timeout!\n",
    "** Looks like i should be using (bucketing)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem:\n",
    "The athena mapping doesn't have many distinct values, a huge number for example have 6061:None:2, while there is only one like this in sql!?\n",
    "The problem, probably was probably address, the concat was buggy and there were too many nulls.\n",
    "Still while raw table matches, donors don't! The athena is too much bigger\n",
    "Start from here: Run this query on both, the results are different"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create table as_blocking_map_number\n",
    "with (bucketed_by = block_number)\n",
    "as( \n",
    "    SELECT donor_id, dense_rank() over (ORDER BY block_key) as block_number\n",
    "    from blocking_map)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "import sys\n",
    "sys.path.insert(0, '../athena_example/')\n",
    "from pyathena import connect\n",
    "from pyathena.pandas_cursor import PandasCursor\n",
    "\n",
    "import config\n",
    "\n",
    "conn = connect(aws_access_key_id=config.ACCESS_KEY_ID,\n",
    "               aws_secret_access_key=config.SECRET_ACCESS_KEY,\n",
    "               s3_staging_dir=config.ATHENA_GARBAGE_PATH,\n",
    "               region_name=config.REGION, \n",
    "               work_group=config.WORKGROUP)    \n",
    "cur = conn.cursor(PandasCursor, schema_name=config.SCHEMA_NAME)\n",
    "q='''\n",
    "with blocking_map_number as( \n",
    "    SELECT donor_id, dense_rank() over (ORDER BY block_key) as block_number\n",
    "    from blocking_map)\n",
    "create table donor_id_pairs as (\n",
    "    SELECT DISTINCT l.donor_id as east, r.donor_id as west\n",
    "    from blocking_map_number as l\n",
    "    INNER JOIN blocking_map_number as r\n",
    "    using (block_number)\n",
    "    where l.donor_id < r.donor_id)\n",
    "'''\n",
    "cur.execute(q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
